{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyN3ywT4f5Xiq9p9lNti/0ys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqbitegh/Colabs/blob/main/phi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ipywidgets"
      ],
      "metadata": {
        "id": "IE9GJiQXen9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import huggingface_hub\n",
        "from huggingface_hub import login, hf_hub_download\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "def init_model():\n",
        "    model_name = \"microsoft/phi-2\" # Replace with the actual model name on Hugging Face Hub\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def init_tinyllama_model():\n",
        "    #model_name = \"TinyLlama/TinyLlama_v1.1_math_code\"\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def init_bielik_1_5b_model():\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token)\n",
        "    model_name = \"speakleash/Bielik-1.5B-v3.0-Instruct\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def init_bielik_1_5b_manual_model():\n",
        "    model_name = \"speakleash/Bielik-1.5B-v3.0-Instruct\"\n",
        "    # Define the directory where files will be downloaded\n",
        "    local_dir = f\"./{model_name.replace('/', '_')}\"\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "    # Specify the files you want to download\n",
        "    files_to_download = [\"config.json\", \"pytorch_model.bin\", \"tokenizer.json\", \"tokenizer_config.json\"] # Add other necessary files\n",
        "\n",
        "    for file_name in files_to_download:\n",
        "        try:\n",
        "            hf_hub_download(repo_id=model_name, filename=file_name, local_dir=local_dir)\n",
        "            print(f\"Downloaded {file_name} to {local_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {file_name}: {e}\")\n",
        "\n",
        "    # Load model and tokenizer from the local directory\n",
        "    model = AutoModelForCausalLM.from_pretrained(local_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def init_bielik_4_5b_model():\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token)\n",
        "\n",
        "    model_name = \"sidorovdy/bielik-4.5b-instruct\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "12MNx7xDsrXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PQyjvqEqvub"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def save_layer_activations(hidden_states, layer_index, output_dir=\"layer_activations\", filename=\"activations.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the hidden states of a specific layer to a text file.\n",
        "\n",
        "    Args:\n",
        "        hidden_states (tuple): Tuple of hidden states from all layers.\n",
        "        layer_index (int): The index of the layer to save activations from.\n",
        "        output_dir (str): The directory to save the output file.\n",
        "        filename (str): The name of the output file.\n",
        "    \"\"\"\n",
        "    if layer_index < len(hidden_states):\n",
        "        layer_activations = hidden_states[layer_index]\n",
        "\n",
        "        # Assuming batch size is 1 and sequence length is the second dimension\n",
        "        # The shape is typically (batch_size, sequence_length, hidden_size)\n",
        "        # We need to save activations for each token in the sequence\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Open in append mode to write activations from different inference calls\n",
        "        with open(file_path, 'a') as f:\n",
        "            # Iterate through each token in the sequence\n",
        "            # Assuming batch size is 1, we access layer_activations[0]\n",
        "            for token_activations in layer_activations[0].cpu().detach().numpy():\n",
        "                np.savetxt(f, token_activations.reshape(1, -1), fmt='%f') # Use numpy.savetxt, reshape for 1D array\n",
        "                f.write('\\n') # Add a newline after each token's activations\n",
        "\n",
        "        print(f\"Activations from layer {layer_index} saved to {file_path}\")\n",
        "    else:\n",
        "        print(f\"Error: Layer index {layer_index} is out of bounds.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_inference(model, tokenizer, input_prompt, activations_filename):\n",
        "    # Determine if a GPU is available and set the device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Move the model to the selected device\n",
        "    model.to(device)\n",
        "\n",
        "    input_text = input_prompt\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device) # Move input tensor to the device\n",
        "\n",
        "    print(f\"tokenizer.eos_token_id : {tokenizer.eos_token_id}\")\n",
        "    # Configure the model to return hidden states\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(input_ids, output_hidden_states=False)\n",
        "        generated_ids = model.generate(input_ids, max_length=150) # You can adjust max_length\n",
        "\n",
        "    #ihidden_states = outputs.hidden_states\n",
        "    #for i, layer_hidden_state in enumerate(ihidden_states):\n",
        "    #    print(f\"  Layer {i}: {layer_hidden_state.shape}\")\n",
        "    #    #print(f\"  Layer {i}: {layer_hidden_state}\")\n",
        "\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(\"\\nGenerated text:\")\n",
        "    print(generated_text)\n",
        "\n",
        "\n",
        "\n",
        "    whole_text = input_text + generated_text\n",
        "    whole_text_ids = tokenizer.encode(whole_text, return_tensors=\"pt\").to(device) # Move whole_text_ids to the device\n",
        "\n",
        "    print(\"whole_text_ids & tokens:\")\n",
        "    token_ids_list = whole_text_ids[0].tolist()\n",
        "\n",
        "    print(\"Index | Token ID | Decoded Token\")\n",
        "    print(\"------|----------|--------------\")\n",
        "\n",
        "    for index, token_id in enumerate(token_ids_list):\n",
        "        decoded_token = tokenizer.decode(token_id)\n",
        "        print(f\"{index:<5} | {token_id:<8} | {decoded_token}\")\n",
        "\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        wholetext_outputs = model(whole_text_ids, output_hidden_states=True)\n",
        "    print(f\"whole_text_ids {whole_text_ids} {whole_text_ids.shape}\")\n",
        "\n",
        "    # Access the hidden states\n",
        "    # outputs.hidden_states is a tuple where each element is the hidden state for a layer\n",
        "    # The first element is the embedding layer output, and the last is the output before the classification head\n",
        "    hidden_states = wholetext_outputs.hidden_states\n",
        "\n",
        "    #print(f\"  Layer {32}: {hidden_states[32].shape}\")\n",
        "    #print(\"Shapes of hidden states for all layers:\")\n",
        "\n",
        "    last_layer = 22 #tinyllama\n",
        "    #last_layer = 32 #phi2\n",
        "    save_layer_activations(hidden_states, last_layer, \"layer_activations\", activations_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYSb_jfKlXpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model, tokenizer = init_model()\n",
        "\n",
        "#model, tokenizer = init_tinyllama_model()\n",
        "model, tokenizer = init_bielik_1_5b_model()\n",
        "#model, tokenizer = init_bielik_1_5b_manual_model()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eDN_x9qarAxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "id": "NmY2ji32BVDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run_inference(model, tokenizer, \"Your input text here\")\n",
        "\n",
        "run_inference(model, tokenizer, \"please write factorial function in c, use reccurence: \", \"activations_tinyllama_factor_c5.txt\")"
      ],
      "metadata": {
        "id": "sYHjdnOjuRHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "system_prompt = \"<|system|> Generate c programs. Use recurrence. </s>\"\n",
        "user_prompt = \"<|user|> please write factorial function.</s>\"\n",
        "assistant_prompt = \"<|assistant|>\""
      ],
      "metadata": {
        "id": "PFE1YgIPTsW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "run_inference(model, tokenizer, system_prompt + user_prompt + assistant_prompt, \"activs_tinyllama_cprrec_fact.txt\")\n",
        "run_inference(model, tokenizer, \"<|system|> Generate c programs. </s>\" + user_prompt + assistant_prompt, \"activs_tinyllama_cpr_fact.txt\")\n",
        "run_inference(model, tokenizer, system_prompt + \"<|user|> please write short haiku about cucumber.</s>\" + assistant_prompt, \"activs_tinyllama_cprrec_cucumb.txt\")\n",
        "run_inference(model, tokenizer, \"<|user|> please write short haiku about cucumber.</s>\" + assistant_prompt, \"activs_tinyllama__cucumb.txt\")\n"
      ],
      "metadata": {
        "id": "l9nXSXbFrALs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference(model, tokenizer, \"Generate c programs.Do not use recurrence.\" + user_prompt + assistant_prompt, \"activs_tinyllama_cnorec_fact.txt\")\n"
      ],
      "metadata": {
        "id": "RxXVAIypTZKq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}