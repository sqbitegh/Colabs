{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTARErCjBo18JnHygiZpLX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqbitegh/Colabs/blob/main/phi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IE9GJiQXen9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "def init_model():\n",
        "    model_name = \"microsoft/phi-2\" # Replace with the actual model name on Hugging Face Hub\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def init_tinyllama_model():\n",
        "    model_name = \"TinyLlama/TinyLlama_v1.1_math_code\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "12MNx7xDsrXQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_PQyjvqEqvub"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def save_layer_activations(hidden_states, layer_index, output_dir=\"layer_activations\", filename=\"activations.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the hidden states of a specific layer to a text file.\n",
        "\n",
        "    Args:\n",
        "        hidden_states (tuple): Tuple of hidden states from all layers.\n",
        "        layer_index (int): The index of the layer to save activations from.\n",
        "        output_dir (str): The directory to save the output file.\n",
        "        filename (str): The name of the output file.\n",
        "    \"\"\"\n",
        "    if layer_index < len(hidden_states):\n",
        "        layer_activations = hidden_states[layer_index]\n",
        "\n",
        "        # Assuming batch size is 1 and sequence length is the second dimension\n",
        "        # The shape is typically (batch_size, sequence_length, hidden_size)\n",
        "        # We need to save activations for each token in the sequence\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Open in append mode to write activations from different inference calls\n",
        "        with open(file_path, 'a') as f:\n",
        "            # Iterate through each token in the sequence\n",
        "            # Assuming batch size is 1, we access layer_activations[0]\n",
        "            for token_activations in layer_activations[0].cpu().detach().numpy():\n",
        "                np.savetxt(f, token_activations.reshape(1, -1), fmt='%f') # Use numpy.savetxt, reshape for 1D array\n",
        "                f.write('\\n') # Add a newline after each token's activations\n",
        "\n",
        "        print(f\"Activations from layer {layer_index} saved to {file_path}\")\n",
        "    else:\n",
        "        print(f\"Error: Layer index {layer_index} is out of bounds.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_inference(model, tokenizer, input_prompt, activations_filename):\n",
        "    input_text = input_prompt\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    print(f\"tokenizer.eos_token_id : {tokenizer.eos_token_id}\")\n",
        "    # Configure the model to return hidden states\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(input_ids, output_hidden_states=False)\n",
        "        generated_ids = model.generate(input_ids, max_length=150) # You can adjust max_length\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(\"\\nGenerated text:\")\n",
        "    print(generated_text)\n",
        "\n",
        "\n",
        "\n",
        "    whole_text = input_text + generated_text\n",
        "    whole_text_ids = tokenizer.encode(whole_text, return_tensors=\"pt\")\n",
        "\n",
        "    print(\"whole_text_ids & tokens:\")\n",
        "    token_ids_list = whole_text_ids[0].tolist()\n",
        "\n",
        "    print(\"Index | Token ID | Decoded Token\")\n",
        "    print(\"------|----------|--------------\")\n",
        "\n",
        "    for index, token_id in enumerate(token_ids_list):\n",
        "        decoded_token = tokenizer.decode(token_id)\n",
        "        print(f\"{index:<5} | {token_id:<8} | {decoded_token}\")\n",
        "\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        wholetext_outputs = model(whole_text_ids, output_hidden_states=True)\n",
        "    print(f\"whole_text_ids {whole_text_ids} {whole_text_ids.shape}\")\n",
        "\n",
        "    # Access the hidden states\n",
        "    # outputs.hidden_states is a tuple where each element is the hidden state for a layer\n",
        "    # The first element is the embedding layer output, and the last is the output before the classification head\n",
        "    hidden_states = wholetext_outputs.hidden_states\n",
        "\n",
        "    print(f\"  Layer {32}: {hidden_states[32].shape}\")\n",
        "    #print(\"Shapes of hidden states for all layers:\")\n",
        "\n",
        "    #for i, layer_hidden_state in enumerate(hidden_states):\n",
        "    #    print(f\"  Layer {i}: {layer_hidden_state.shape}\")\n",
        "    #    #print(f\"  Layer {i}: {layer_hidden_state}\")\n",
        "\n",
        "    save_layer_activations(hidden_states, 32, \"layer_activations\", activations_filename)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYSb_jfKlXpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = init_model()\n",
        "\n",
        "model, tokenizer = init_tinyllama_model()\n"
      ],
      "metadata": {
        "id": "eDN_x9qarAxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmY2ji32BVDO",
        "outputId": "ba1c19d7-f5c3-427a-847c-83754b055655"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhiConfig {\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2560,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 10240,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.4,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run_inference(model, tokenizer, \"Your input text here\")\n",
        "\n",
        "run_inference(model, tokenizer, \"example factorial function in c: \", \"activations_phi2_factor_c3.txt\")"
      ],
      "metadata": {
        "id": "sYHjdnOjuRHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}