{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKjMF3EGYP8hVZAuQdbhUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqbitegh/Colabs/blob/main/phi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IE9GJiQXen9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "def init_model():\n",
        "    model_name = \"microsoft/phi-2\" # Replace with the actual model name on Hugging Face Hub\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def init_tinyllama_model():\n",
        "    #model_name = \"TinyLlama/TinyLlama_v1.1_math_code\"\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "12MNx7xDsrXQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_PQyjvqEqvub"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def save_layer_activations(hidden_states, layer_index, output_dir=\"layer_activations\", filename=\"activations.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the hidden states of a specific layer to a text file.\n",
        "\n",
        "    Args:\n",
        "        hidden_states (tuple): Tuple of hidden states from all layers.\n",
        "        layer_index (int): The index of the layer to save activations from.\n",
        "        output_dir (str): The directory to save the output file.\n",
        "        filename (str): The name of the output file.\n",
        "    \"\"\"\n",
        "    if layer_index < len(hidden_states):\n",
        "        layer_activations = hidden_states[layer_index]\n",
        "\n",
        "        # Assuming batch size is 1 and sequence length is the second dimension\n",
        "        # The shape is typically (batch_size, sequence_length, hidden_size)\n",
        "        # We need to save activations for each token in the sequence\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Open in append mode to write activations from different inference calls\n",
        "        with open(file_path, 'a') as f:\n",
        "            # Iterate through each token in the sequence\n",
        "            # Assuming batch size is 1, we access layer_activations[0]\n",
        "            for token_activations in layer_activations[0].cpu().detach().numpy():\n",
        "                np.savetxt(f, token_activations.reshape(1, -1), fmt='%f') # Use numpy.savetxt, reshape for 1D array\n",
        "                f.write('\\n') # Add a newline after each token's activations\n",
        "\n",
        "        print(f\"Activations from layer {layer_index} saved to {file_path}\")\n",
        "    else:\n",
        "        print(f\"Error: Layer index {layer_index} is out of bounds.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_inference(model, tokenizer, input_prompt, activations_filename):\n",
        "    # Determine if a GPU is available and set the device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Move the model to the selected device\n",
        "    model.to(device)\n",
        "\n",
        "    input_text = input_prompt\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device) # Move input tensor to the device\n",
        "\n",
        "    print(f\"tokenizer.eos_token_id : {tokenizer.eos_token_id}\")\n",
        "    # Configure the model to return hidden states\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(input_ids, output_hidden_states=False)\n",
        "        generated_ids = model.generate(input_ids, max_length=150) # You can adjust max_length\n",
        "\n",
        "    #ihidden_states = outputs.hidden_states\n",
        "    #for i, layer_hidden_state in enumerate(ihidden_states):\n",
        "    #    print(f\"  Layer {i}: {layer_hidden_state.shape}\")\n",
        "    #    #print(f\"  Layer {i}: {layer_hidden_state}\")\n",
        "\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(\"\\nGenerated text:\")\n",
        "    print(generated_text)\n",
        "\n",
        "\n",
        "\n",
        "    whole_text = input_text + generated_text\n",
        "    whole_text_ids = tokenizer.encode(whole_text, return_tensors=\"pt\").to(device) # Move whole_text_ids to the device\n",
        "\n",
        "    print(\"whole_text_ids & tokens:\")\n",
        "    token_ids_list = whole_text_ids[0].tolist()\n",
        "\n",
        "    print(\"Index | Token ID | Decoded Token\")\n",
        "    print(\"------|----------|--------------\")\n",
        "\n",
        "    for index, token_id in enumerate(token_ids_list):\n",
        "        decoded_token = tokenizer.decode(token_id)\n",
        "        print(f\"{index:<5} | {token_id:<8} | {decoded_token}\")\n",
        "\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        wholetext_outputs = model(whole_text_ids, output_hidden_states=True)\n",
        "    print(f\"whole_text_ids {whole_text_ids} {whole_text_ids.shape}\")\n",
        "\n",
        "    # Access the hidden states\n",
        "    # outputs.hidden_states is a tuple where each element is the hidden state for a layer\n",
        "    # The first element is the embedding layer output, and the last is the output before the classification head\n",
        "    hidden_states = wholetext_outputs.hidden_states\n",
        "\n",
        "    #print(f\"  Layer {32}: {hidden_states[32].shape}\")\n",
        "    #print(\"Shapes of hidden states for all layers:\")\n",
        "\n",
        "    last_layer = 22 #tinyllama\n",
        "    #last_layer = 32 #phi2\n",
        "    save_layer_activations(hidden_states, last_layer, \"layer_activations\", activations_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYSb_jfKlXpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model, tokenizer = init_model()\n",
        "\n",
        "model, tokenizer = init_tinyllama_model()\n"
      ],
      "metadata": {
        "id": "eDN_x9qarAxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "id": "NmY2ji32BVDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff59c724-68bf-4c03-c503-82f7e2f4ce69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.52.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run_inference(model, tokenizer, \"Your input text here\")\n",
        "\n",
        "run_inference(model, tokenizer, \"please write factorial function in c, use reccurence: \", \"activations_tinyllama_factor_c5.txt\")"
      ],
      "metadata": {
        "id": "sYHjdnOjuRHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "system_prompt = \"<|system|> Generate c programs. Use recurrence. </s>\"\n",
        "user_prompt = \"<|user|> please write factorial function.</s>\"\n",
        "assistant_prompt = \"<|assistant|>\"\n",
        "run_inference(model, tokenizer, system_prompt + user_prompt + assistant_prompt, \"activs_tinyllama_cprrec_fact.txt\")\n",
        "run_inference(model, tokenizer, \"<|system|> Generate c programs. </s>\" + user_prompt + assistant_prompt, \"activs_tinyllama_cpr_fact.txt\")\n",
        "run_inference(model, tokenizer, system_prompt + \"<|user|> please write short haiku about cucumber.</s>\" + assistant_prompt, \"activs_tinyllama_cprrec_cucumb.txt\")\n",
        "run_inference(model, tokenizer, \"<|user|> please write short haiku about cucumber.</s>\" + assistant_prompt, \"activs_tinyllama__cucumb.txt\")\n"
      ],
      "metadata": {
        "id": "l9nXSXbFrALs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}