{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhlbtkCOShAuHGWuiSljFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqbitegh/Colabs/blob/main/MiniNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install torchtext==0.15.1 --no-cache-dir\n",
        "!pip install numpy==1.24.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RXD5nWL2yAnC",
        "outputId": "a9526ba3-5c07-4e06-9707-27fdc0d18f53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-3.1.1\n",
            "Collecting torchtext==0.15.1\n",
            "  Downloading torchtext-0.15.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (2.32.3)\n",
            "Collecting torch==2.0.0 (from torchtext==0.15.1)\n",
            "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (2.0.2)\n",
            "Collecting torchdata==0.6.0 (from torchtext==0.15.1)\n",
            "  Downloading torchdata-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (892 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (4.13.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n",
            "Downloading torchtext-0.15.1-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m224.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m200.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m215.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m347.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m250.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m143.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m138.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m180.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m213.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1 triton-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "554ba752856f45c49b80dc41ac5f3bbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.1\n",
            "  Downloading numpy-1.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.1 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.1 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.0 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.24.1 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.1 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.1 which is incompatible.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.1 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9c533bae69114447a6e18d7a2e409a3e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "#!pip install --upgrade torchtext"
      ],
      "metadata": {
        "id": "aqkshcLZxkwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import portalocker\n",
        "import numpy\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "8JebT1cowfZj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(torch.__version__)\n",
        "print(torchtext.__version__)\n",
        "print(portalocker.__version__)\n",
        "print(numpy.__version__)\n",
        "\n",
        "\"\"\"\n",
        "expected\n",
        "2.0.0+cu117\n",
        "0.15.1+cpu\n",
        "3.1.1\n",
        "1.24.1\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "5hDOK1VRzE3E",
        "outputId": "0f653068-7f74-44f4-ada0-c829e0662b2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu117\n",
            "0.15.1+cpu\n",
            "3.1.1\n",
            "1.24.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nexpected\\n2.0.0+cu117\\n0.15.1+cpu\\n3.1.1\\n1.24.1\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tokenizer and Vocabulary\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter, test_iter = IMDB()\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        #print(f\"rawlabel {_label} {_text}\")\n",
        "        label_list.append(_label - 1)\n",
        "        processed_text = torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "    return torch.tensor(label_list, dtype=torch.int64), nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_iter, test_iter = IMDB()\n",
        "train_data = list(train_iter)\n",
        "train_data = [(label, text[:1000]) for label, text in train_iter]\n",
        "random.shuffle(train_data)\n",
        "\n",
        "train_size = int(0.8 * len(train_data))\n",
        "train_data, test_data = train_data[:train_size], train_data[train_size:] # Splitting training data into train and validation\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_batch) # Shuffle training data\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_batch)\n",
        "\n",
        "\"\"\"\n",
        "print(\"test_data print:\")\n",
        "for dp in test_data:\n",
        "  print(dp)\n",
        "print(\"test_data print end.\")\n",
        "\"\"\"\n",
        "# Single-Head Transformer Model\n",
        "class SingleHeadTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_class):\n",
        "        super(SingleHeadTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, embed_size)) #learnable position encodings\n",
        "        #self.pos_encoder =self.positional_encoding(embed_size)  # Use fixed encoding\n",
        "        # Cache for trimmed positional encodings\n",
        "        self.trimmed_pos_encoder_cache = {}\n",
        "        self.attention = nn.MultiheadAttention(embed_size, num_heads=2)\n",
        "        self.fc = nn.Linear(embed_size, num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        # Slice the pre-computed positional encodings\n",
        "        seq_len = x.shape[1]\n",
        "        batch_len = x.shape[0]\n",
        "\n",
        "        #print(f\"seq_len: {seq_len}, batch_len: {batch_len}\")\n",
        "        if seq_len in self.trimmed_pos_encoder_cache and batch_len == 32:\n",
        "            trimmed_pos_encoder = self.trimmed_pos_encoder_cache[seq_len]\n",
        "            trimmed_pos_encoder = trimmed_pos_encoder.expand(x.size(0), *trimmed_pos_encoder.size()[1:]) #add batch size\n",
        "            #print(f\"found in cache x.shape: {x.shape}, trimmed_pos_encoder.shape: {trimmed_pos_encoder.shape}\")\n",
        "        else:\n",
        "          trimmed_pos_encoder = self.pos_encoder[:seq_len, 0, :].unsqueeze(0).to(x.device) # Updated to remove middle dimension\n",
        "          #print(f\"self.pos_encoder.shape: {self.pos_encoder.shape}, trimmed_pos_encoder.shape: {trimmed_pos_encoder.shape}\")\n",
        "          trimmed_pos_encoder = trimmed_pos_encoder.expand(x.size(0), *trimmed_pos_encoder.size()[1:]) #add batch size\n",
        "          #print(f\"x.shape: {x.shape}, trimmed_pos_encoder.shape: {trimmed_pos_encoder.shape}\")\n",
        "          if batch_len == 32:\n",
        "            #print(\"adding to cache\")\n",
        "            self.trimmed_pos_encoder_cache[seq_len] = trimmed_pos_encoder  # Store in cache\n",
        "\n",
        "        #print(f\"x.shape: {x.shape}, trimmed_pos_encoder.shape: {trimmed_pos_encoder.shape}\")\n",
        "        x = x + trimmed_pos_encoder  # Add positional encodings\n",
        "        \"\"\"\n",
        "\n",
        "        x = x.transpose(0, 1)  # Transform to (seq_len, batch_size, embed_size) for attention\n",
        "        x, _ = self.attention(x, x, x)\n",
        "        x = x.mean(dim=0)  # Aggregate over sequence length\n",
        "        return self.fc(x)\n",
        "\n",
        "    def positional_encoding(self, d_model, max_len=5000):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        return pe\n",
        "\n",
        "# Instantiate model, loss, optimizer\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 2*64\n",
        "num_class = 2\n",
        "model = SingleHeadTransformer(vocab_size, embed_size, num_class).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Function\n",
        "def train(dataloader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    for labels, texts in dataloader:\n",
        "        labels, texts = labels.to(device), texts.to(device)\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation Function with Activation Logging\n",
        "def evaluate_and_log_activations(dataloader, model):\n",
        "    model.eval()\n",
        "    activations = []\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output.cpu().detach().numpy())\n",
        "        #activations.append(output.numpy())\n",
        "\n",
        "    #handle = model.fc.register_forward_hook(forward_hook)\n",
        "\n",
        "    corrects, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for labels, texts in dataloader:\n",
        "            #print(f\"labels: {labels}\")\n",
        "            labels, texts = labels.to(device), texts.to(device)\n",
        "            outputs = model(texts)\n",
        "            #print(f\"outputs: {outputs}\")\n",
        "            #print(f\"labels: {labels}\")\n",
        "\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            corrects += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "            # Log each activation for each data point\n",
        "            # Note: This stores the last batch activations\n",
        "            #print(activations[-1])\n",
        "            #print(f\"Activation shape: {len(activations[-1])}\")\n",
        "            # Dump activations to file\n",
        "            \"\"\"\n",
        "            with open(f'activations_epoch_{epoch + 1}.txt', 'w') as f:\n",
        "              for activation in activations[-1]:\n",
        "                f.write(str(activation) + '\\n')\n",
        "              f.write(\"end\")\n",
        "            \"\"\"\n",
        "    #print(len(dataloader.dataset))\n",
        "    accuracy = corrects / len(dataloader.dataset)\n",
        "    #handle.remove()\n",
        "    print(f\"Dataset size: {len(dataloader.dataset)}, Dataloader length: {len(dataloader)}, Accuracy: {accuracy:.4f}, Corrects: {corrects}, total_loss {total_loss}\") # Modified: Print formatting\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "def cleanup_memory():\n",
        "  #del model\n",
        "  del train_dataloader\n",
        "  del test_dataloader\n",
        "  del train_data\n",
        "  del test_data\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 120\n",
        "epoch = 0\n",
        "\n",
        "val_loss, val_accuracy = evaluate_and_log_activations(train_dataloader, model)\n",
        "print(f'Epoch {0}, train Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "val_loss, val_accuracy = evaluate_and_log_activations(test_dataloader, model)\n",
        "print(f'Epoch {0}, test Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "print(f\"train_dataloader size: {len(train_dataloader.dataset)}\")\n",
        "print(f\"test_dataloader size: {len(test_dataloader.dataset)}\")\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "for epoch in range(num_epochs):\n",
        "    train(train_dataloader, model, criterion, optimizer)\n",
        "    val_loss, val_accuracy = evaluate_and_log_activations(train_dataloader, model)\n",
        "    print(f'train Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "    val_loss, val_accuracy = evaluate_and_log_activations(test_dataloader, model)\n",
        "    print(f'test Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "    scheduler.step()\n",
        "print(\"Training complete.\")\n",
        "cleanup_memory()\n",
        "print(\"Cleaning complete.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8fOsi0d1wYfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "089bc2f8-c696-4862-bfbc-698bb816bfc5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.4718, Corrects: 9435, total_loss 433.9523043036461\n",
            "Epoch 0, train Val Loss: 0.6943, Val Accuracy: 0.4718\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.4818, Corrects: 2409, total_loss 108.95529228448868\n",
            "Epoch 0, test Val Loss: 0.6940, Val Accuracy: 0.4818\n",
            "train_dataloader size: 20000\n",
            "test_dataloader size: 5000\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.8556, Corrects: 17112, total_loss 219.3590003848076\n",
            "train Epoch 1, Val Loss: 0.3510, Val Accuracy: 0.8556\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8282, Corrects: 4141, total_loss 61.42513366043568\n",
            "test Epoch 1, Val Loss: 0.3912, Val Accuracy: 0.8282\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9074, Corrects: 18149, total_loss 143.502851780504\n",
            "train Epoch 2, Val Loss: 0.2296, Val Accuracy: 0.9074\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8378, Corrects: 4189, total_loss 59.1366238668561\n",
            "test Epoch 2, Val Loss: 0.3767, Val Accuracy: 0.8378\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9482, Corrects: 18964, total_loss 88.63360118400306\n",
            "train Epoch 3, Val Loss: 0.1418, Val Accuracy: 0.9482\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8404, Corrects: 4202, total_loss 68.94804564863443\n",
            "test Epoch 3, Val Loss: 0.4392, Val Accuracy: 0.8404\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9666, Corrects: 19332, total_loss 59.27386327506974\n",
            "train Epoch 4, Val Loss: 0.0948, Val Accuracy: 0.9666\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8506, Corrects: 4253, total_loss 71.33058675006032\n",
            "test Epoch 4, Val Loss: 0.4543, Val Accuracy: 0.8506\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9728, Corrects: 19455, total_loss 50.414087320212275\n",
            "train Epoch 5, Val Loss: 0.0807, Val Accuracy: 0.9728\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8504, Corrects: 4252, total_loss 76.38937485963106\n",
            "test Epoch 5, Val Loss: 0.4866, Val Accuracy: 0.8504\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9773, Corrects: 19546, total_loss 43.25918744481169\n",
            "train Epoch 6, Val Loss: 0.0692, Val Accuracy: 0.9773\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8490, Corrects: 4245, total_loss 81.90703127160668\n",
            "test Epoch 6, Val Loss: 0.5217, Val Accuracy: 0.8490\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9780, Corrects: 19560, total_loss 42.039915106026456\n",
            "train Epoch 7, Val Loss: 0.0673, Val Accuracy: 0.9780\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8494, Corrects: 4247, total_loss 82.15678618475795\n",
            "test Epoch 7, Val Loss: 0.5233, Val Accuracy: 0.8494\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9785, Corrects: 19569, total_loss 41.18804284906946\n",
            "train Epoch 8, Val Loss: 0.0659, Val Accuracy: 0.9785\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8486, Corrects: 4243, total_loss 82.8689770847559\n",
            "test Epoch 8, Val Loss: 0.5278, Val Accuracy: 0.8486\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9790, Corrects: 19580, total_loss 40.37992525147274\n",
            "train Epoch 9, Val Loss: 0.0646, Val Accuracy: 0.9790\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 83.75108355283737\n",
            "test Epoch 9, Val Loss: 0.5334, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9792, Corrects: 19585, total_loss 40.1969888699241\n",
            "train Epoch 10, Val Loss: 0.0643, Val Accuracy: 0.9792\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8490, Corrects: 4245, total_loss 83.81026776880026\n",
            "test Epoch 10, Val Loss: 0.5338, Val Accuracy: 0.8490\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9797, Corrects: 19594, total_loss 40.08351484849118\n",
            "train Epoch 11, Val Loss: 0.0641, Val Accuracy: 0.9797\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 83.89165840670466\n",
            "test Epoch 11, Val Loss: 0.5343, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9799, Corrects: 19598, total_loss 39.991195196751505\n",
            "train Epoch 12, Val Loss: 0.0640, Val Accuracy: 0.9799\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 83.97816680371761\n",
            "test Epoch 12, Val Loss: 0.5349, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9799, Corrects: 19599, total_loss 39.98265256546438\n",
            "train Epoch 13, Val Loss: 0.0640, Val Accuracy: 0.9799\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 83.98696698993444\n",
            "test Epoch 13, Val Loss: 0.5349, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.97420649812557\n",
            "train Epoch 14, Val Loss: 0.0640, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 83.99581292271614\n",
            "test Epoch 14, Val Loss: 0.5350, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96585776936263\n",
            "train Epoch 15, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00468481704593\n",
            "test Epoch 15, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96537959179841\n",
            "train Epoch 16, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00519673526287\n",
            "test Epoch 16, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96490082051605\n",
            "train Epoch 17, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00571372359991\n",
            "test Epoch 17, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96442346833646\n",
            "train Epoch 18, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623171404004\n",
            "test Epoch 18, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96441678307019\n",
            "train Epoch 19, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623141974211\n",
            "test Epoch 19, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.964410041226074\n",
            "train Epoch 20, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.0062313824892\n",
            "test Epoch 20, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440257085487\n",
            "train Epoch 21, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623041763902\n",
            "test Epoch 21, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440246514976\n",
            "train Epoch 22, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623056665063\n",
            "test Epoch 22, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440262766555\n",
            "train Epoch 23, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623075664043\n",
            "test Epoch 23, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.964402419514954\n",
            "train Epoch 24, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623099505901\n",
            "test Epoch 24, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440244046971\n",
            "train Epoch 25, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623117387295\n",
            "test Epoch 25, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440242230892\n",
            "train Epoch 26, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623131543398\n",
            "test Epoch 26, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 27, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623122602701\n",
            "test Epoch 27, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 28, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 28, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 29, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 29, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 30, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 30, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 31, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 31, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 32, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 32, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 33, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 33, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 34, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 34, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 35, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 35, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 36, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 36, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 37, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 37, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 38, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 38, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 39, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 39, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 40, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 40, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 41, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 41, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 42, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 42, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 43, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 43, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 44, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 44, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 45, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 45, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 46, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 46, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 47, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 47, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 48, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 48, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 49, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 49, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 50, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 50, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 51, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 51, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 52, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 52, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 53, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 53, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 54, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 54, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 55, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 55, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 56, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 56, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 57, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 57, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 58, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 58, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 59, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 59, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 60, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 60, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 61, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 61, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 62, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 62, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 63, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 63, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 64, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 64, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 65, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 65, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 66, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 66, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 67, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 67, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 68, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 68, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 69, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 69, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 70, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 70, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 71, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 71, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 72, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 72, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 73, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 73, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 74, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 74, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 75, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 75, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 76, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 76, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 77, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 77, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 78, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 78, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 79, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 79, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 80, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 80, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 81, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 81, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 82, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 82, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 83, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 83, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 84, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 84, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 85, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 85, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 86, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 86, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 87, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 87, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 88, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 88, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 89, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 89, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 90, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 90, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 91, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 91, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 92, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 92, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 93, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 93, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 94, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 94, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 95, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 95, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 96, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 96, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 97, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 97, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 98, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 98, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 99, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 99, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 100, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 100, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 101, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 101, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 102, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 102, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 103, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 103, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 104, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 104, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 105, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 105, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 106, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 106, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 107, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 107, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 108, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 108, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 109, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 109, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 110, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 110, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 111, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 111, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 112, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 112, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 113, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 113, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 114, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 114, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 115, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 115, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 116, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 116, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 117, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 117, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 118, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 118, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 119, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 119, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9800, Corrects: 19600, total_loss 39.96440245676786\n",
            "train Epoch 120, Val Loss: 0.0639, Val Accuracy: 0.9800\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8492, Corrects: 4246, total_loss 84.00623116642237\n",
            "test Epoch 120, Val Loss: 0.5351, Val Accuracy: 0.8492\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'train_dataloader' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-861f4ab6e081>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0mcleanup_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cleaning complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-861f4ab6e081>\u001b[0m in \u001b[0;36mcleanup_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleanup_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0;31m#del model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m   \u001b[0;32mdel\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'train_dataloader' where it is not associated with a value"
          ]
        }
      ]
    }
  ]
}