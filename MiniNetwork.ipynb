{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTlPupIZJLDRGqKhG3p1Sa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqbitegh/Colabs/blob/main/MiniNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install torchtext==0.15.1 --no-cache-dir\n",
        "!pip install numpy==1.24.1"
      ],
      "metadata": {
        "id": "RXD5nWL2yAnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "#!pip install --upgrade torchtext"
      ],
      "metadata": {
        "id": "aqkshcLZxkwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import portalocker\n",
        "import numpy\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "8JebT1cowfZj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(torch.__version__)\n",
        "print(torchtext.__version__)\n",
        "print(portalocker.__version__)\n",
        "print(numpy.__version__)\n",
        "\n",
        "\"\"\"\n",
        "expected\n",
        "2.0.0+cu117\n",
        "0.15.1+cpu\n",
        "3.1.1\n",
        "1.24.1\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "5hDOK1VRzE3E",
        "outputId": "5d0e28b5-276a-4674-ff67-7cd56569191d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu117\n",
            "0.15.1+cpu\n",
            "3.1.1\n",
            "1.24.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nexpected\\n2.0.0+cu117\\n0.15.1+cpu\\n3.1.1\\n1.24.1\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tokenizer and Vocabulary\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter, test_iter = IMDB()\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        #print(f\"rawlabel {_label} {_text}\")\n",
        "        label_list.append(_label - 1)\n",
        "        processed_text = torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "    return torch.tensor(label_list, dtype=torch.int64), nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_iter, test_iter = IMDB()\n",
        "train_data = list(train_iter)\n",
        "random.shuffle(train_data)\n",
        "\n",
        "train_size = int(0.8 * len(train_data))\n",
        "train_data, test_data = train_data[:train_size], train_data[train_size:] # Splitting training data into train and validation\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_batch, shuffle=True) # Shuffle training data\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_batch, shuffle=True)\n",
        "\n",
        "\"\"\"\n",
        "print(\"test_data print:\")\n",
        "for dp in test_data:\n",
        "  print(dp)\n",
        "print(\"test_data print end.\")\n",
        "\"\"\"\n",
        "# Single-Head Transformer Model\n",
        "class SingleHeadTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_class):\n",
        "        super(SingleHeadTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        #self.pos_encoder = nn.Parameter(torch.zeros(1, embed_size)) #learnable position encodings\n",
        "        self.pos_encoder =self.positional_encoding(embed_size)  # Use fixed encoding\n",
        "        # Cache for trimmed positional encodings\n",
        "        self.trimmed_pos_encoder_cache = {}\n",
        "        self.attention = nn.MultiheadAttention(embed_size, num_heads=1)\n",
        "        self.fc = nn.Linear(embed_size, num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.embedding(x) + self.pos_encoder\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        # Slice the pre-computed positional encodings\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        if seq_len in self.trimmed_pos_encoder_cache:\n",
        "            trimmed_pos_encoder = self.trimmed_pos_encoder_cache[seq_len]\n",
        "        else:\n",
        "          trimmed_pos_encoder = self.pos_encoder[:seq_len, 0, :].unsqueeze(0).to(x.device) # Updated to remove middle dimension\n",
        "          #print(f\"self.pos_encoder.shape: {self.pos_encoder.shape}, trimmed_pos_encoder.shape: {trimmed_pos_encoder.shape}\")\n",
        "          trimmed_pos_encoder = trimmed_pos_encoder.expand(x.size(0), *trimmed_pos_encoder.size()[1:]) #add batch size\n",
        "          #print(f\"x.shape: {x.shape}, trimmed_pos_encoder.shape: {trimmed_pos_encoder.shape}\")\n",
        "          self.trimmed_pos_encoder_cache[seq_len] = trimmed_pos_encoder  # Store in cache\n",
        "\n",
        "        x = x + trimmed_pos_encoder  # Add positional encodings\n",
        "\n",
        "\n",
        "        x = x.transpose(0, 1)  # Transform to (seq_len, batch_size, embed_size) for attention\n",
        "        x, _ = self.attention(x, x, x)\n",
        "        x = x.mean(dim=0)  # Aggregate over sequence length\n",
        "        return self.fc(x)\n",
        "    def positional_encoding(self, d_model, max_len=5000):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        return pe\n",
        "\n",
        "# Instantiate model, loss, optimizer\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 4*64\n",
        "num_class = 2\n",
        "model = SingleHeadTransformer(vocab_size, embed_size, num_class).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Function\n",
        "def train(dataloader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    for labels, texts in dataloader:\n",
        "        labels, texts = labels.to(device), texts.to(device)\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation Function with Activation Logging\n",
        "def evaluate_and_log_activations(dataloader, model):\n",
        "    model.eval()\n",
        "    activations = []\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output.cpu().detach().numpy())\n",
        "        #activations.append(output.numpy())\n",
        "\n",
        "    handle = model.fc.register_forward_hook(forward_hook)\n",
        "\n",
        "    corrects, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for labels, texts in dataloader:\n",
        "            #print(f\"labels: {labels}\")\n",
        "            labels, texts = labels.to(device), texts.to(device)\n",
        "            outputs = model(texts)\n",
        "            #print(f\"outputs: {outputs}\")\n",
        "            #print(f\"labels: {labels}\")\n",
        "\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            corrects += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "            # Log each activation for each data point\n",
        "            # Note: This stores the last batch activations\n",
        "            #print(activations[-1])\n",
        "            #print(f\"Activation shape: {len(activations[-1])}\")\n",
        "            # Dump activations to file\n",
        "            with open(f'activations_epoch_{epoch + 1}.txt', 'w') as f:\n",
        "              for activation in activations[-1]:\n",
        "                f.write(str(activation) + '\\n')\n",
        "              f.write(\"end\")\n",
        "\n",
        "    print(len(dataloader.dataset))\n",
        "    accuracy = corrects / len(dataloader.dataset)\n",
        "    handle.remove()\n",
        "    print(f\"Dataset size: {len(dataloader.dataset)}, Dataloader length: {len(dataloader)}, Accuracy: {accuracy:.4f}, Corrects: {corrects}, total_loss {total_loss}\") # Modified: Print formatting\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 25\n",
        "epoch = 0\n",
        "\n",
        "val_loss, val_accuracy = evaluate_and_log_activations(train_dataloader, model)\n",
        "print(f'Epoch {0}, train Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "val_loss, val_accuracy = evaluate_and_log_activations(test_dataloader, model)\n",
        "print(f'Epoch {0}, test Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "print(f\"train_dataloader size: {len(train_dataloader.dataset)}\")\n",
        "print(f\"test_dataloader size: {len(test_dataloader.dataset)}\")\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train(train_dataloader, model, criterion, optimizer)\n",
        "    val_loss, val_accuracy = evaluate_and_log_activations(train_dataloader, model)\n",
        "    print(f'train Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "    val_loss, val_accuracy = evaluate_and_log_activations(test_dataloader, model)\n",
        "    print(f'test Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "8fOsi0d1wYfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1776e128-88d1-412c-aa23-35c2db46d1a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.4979, Corrects: 9958, total_loss 433.9798508286476\n",
            "Epoch 0, train Val Loss: 0.6944, Val Accuracy: 0.4979\n",
            "5000\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.4850, Corrects: 2425, total_loss 109.04482924938202\n",
            "Epoch 0, test Val Loss: 0.6946, Val Accuracy: 0.4850\n",
            "train_dataloader size: 20000\n",
            "test_dataloader size: 5000\n",
            "20000\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.7232, Corrects: 14465, total_loss 345.3813387453556\n",
            "train Epoch 1, Val Loss: 0.5526, Val Accuracy: 0.7232\n",
            "5000\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.7184, Corrects: 3592, total_loss 87.05005899071693\n",
            "test Epoch 1, Val Loss: 0.5545, Val Accuracy: 0.7184\n",
            "20000\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.7891, Corrects: 15782, total_loss 278.82630920410156\n",
            "train Epoch 2, Val Loss: 0.4461, Val Accuracy: 0.7891\n",
            "5000\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.7744, Corrects: 3872, total_loss 74.43166959285736\n",
            "test Epoch 2, Val Loss: 0.4741, Val Accuracy: 0.7744\n",
            "20000\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.8501, Corrects: 17003, total_loss 216.98638187348843\n",
            "train Epoch 3, Val Loss: 0.3472, Val Accuracy: 0.8501\n",
            "5000\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8080, Corrects: 4040, total_loss 66.07625828683376\n",
            "test Epoch 3, Val Loss: 0.4209, Val Accuracy: 0.8080\n",
            "20000\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.8929, Corrects: 17857, total_loss 162.6487617455423\n",
            "train Epoch 4, Val Loss: 0.2602, Val Accuracy: 0.8929\n",
            "5000\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8188, Corrects: 4094, total_loss 63.98631030321121\n",
            "test Epoch 4, Val Loss: 0.4076, Val Accuracy: 0.8188\n",
            "20000\n",
            "Dataset size: 20000, Dataloader length: 625, Accuracy: 0.9288, Corrects: 18577, total_loss 128.46560925245285\n",
            "train Epoch 5, Val Loss: 0.2055, Val Accuracy: 0.9288\n",
            "5000\n",
            "Dataset size: 5000, Dataloader length: 157, Accuracy: 0.8316, Corrects: 4158, total_loss 59.84186166524887\n",
            "test Epoch 5, Val Loss: 0.3812, Val Accuracy: 0.8316\n",
            "Training complete.\n"
          ]
        }
      ]
    }
  ]
}