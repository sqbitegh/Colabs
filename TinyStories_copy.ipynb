{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqbitegh/Colabs/blob/main/TinyStories_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "#preparations:\n",
        "#in /dataset put train.pt, validation.pt\n",
        "#/my_dataset/train and /my_dataset/validation put parquet\n",
        "#/my_tokenizer/ put special_tokens_map.json, tokenizer.json, tokenizer_config.json\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQRqZ23jjwsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clear GPU\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEEOI8ezPn6l"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"hgf token\""
      ],
      "metadata": {
        "id": "eu2LRNzrm98Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass, fields\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import datasets\n",
        "import tokenizers\n",
        "import transformers  # for AutoTokenizer, using our own transformer implementation.\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#saving activations\n",
        "import numpy as np\n",
        "\n",
        "epoch = 0\n",
        "saved_lines = 0\n",
        "lines_to_save_per_epoch = 10000\n",
        "def save_inputs_to_file(module, input, output):\n",
        "    global epoch\n",
        "    global lines_to_save_per_epoch\n",
        "    global saved_lines\n",
        "    if(epoch % 10 != 0):\n",
        "      return\n",
        "\n",
        "    if saved_lines >= lines_to_save_per_epoch:\n",
        "      return\n",
        "    os.makedirs('lm_head_inputs', exist_ok=True)\n",
        "    input_tensor = input[0].cpu().detach().numpy()  # Move to CPU and detach\n",
        "\n",
        "    file_path = os.path.join('lm_head_inputs', f'input_epoch_{epoch}.txt')\n",
        "\n",
        "    with open(file_path, 'a') as f:  # Open in append mode\n",
        "        for data in input_tensor:\n",
        "            np.savetxt(f, data, fmt='%f')  # Use numpy.savetxt\n",
        "            f.write('\\n')\n",
        "    saved_lines += input_tensor.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 128 # max sequence length\n",
        "    n_layer: int = 8 #12 # number of layers\n",
        "    n_head: int = 1 #12 # number of heads\n",
        "    n_embd: int = 384 #384 # embedding dimension\n",
        "    feed_forward_factor: float = 2.0 #1.8  # how much bigger the MLP blocks are than the model n_embd.  Conventionally 4.\n",
        "    vocab_size: int = 8192\n",
        "\n",
        "    data_dir: str = 'dataset'\n",
        "    expt_name: str = '384_dims_is_all_u_need'\n",
        "\n",
        "    batch_size: int = 256 #128\n",
        "    max_lr: float = 2e-3\n",
        "    min_lr: float = 2e-4\n",
        "    beta_1: float = 0.9\n",
        "    beta_2: float = 0.99\n",
        "    warmup_steps:int = 50\n",
        "    max_steps: int = 12000\n",
        "    max_runtime_seconds: int = 60*120 #720\n",
        "\n",
        "    weight_decay: float = 0.12\n",
        "\n",
        "    need_epoch_reshuffle: bool = False\n",
        "    matmul_precision: str = 'high' # medium, high, highest.\n",
        "    # Do various hacky things (don't use torch.compile, don't load training data) to speed up the run.\n",
        "    # # We are checking for runnability rather than model quality.\n",
        "    smoke_test: bool = False\n",
        "\n",
        "    def __str__(self):\n",
        "        return '\\n'.join([f'{field.name}: {str(getattr(self, field.name))}' for field in fields(self)])\n",
        "\n",
        "\n",
        "class Logger():\n",
        "    def __init__(self, expt_name, smoke_test):\n",
        "        self.log_dir = f'logs/{expt_name}{\"_smoke\" if smoke_test else \"\"}'\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.log_file = f'{self.log_dir}/log.txt'\n",
        "        with open(self.log_file, \"w\") as f:\n",
        "            f.write(\"\")\n",
        "\n",
        "    def log(self, msg):\n",
        "        print(msg)\n",
        "        with open(self.log_file, \"a\") as f:\n",
        "            f.write(f\"{msg}\\n\")\n",
        "\n",
        "config = GPTConfig()\n",
        "logger = Logger(os.path.join(config.expt_name), config.smoke_test) # open for writing to clear the file\n",
        "logger.log(str(config))\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        ff_exp = int(config.feed_forward_factor * config.n_embd)\n",
        "        ff_exp -= ff_exp % 64\n",
        "        assert ff_exp % 64 == 0\n",
        "        self.c_fc    = nn.Linear(config.n_embd, ff_exp)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(ff_exp, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "        self.activation_hook = self.lm_head.register_forward_hook(save_inputs_to_file)\n",
        "\n",
        "    def deregister_activation_hook(self):\n",
        "        \"\"\"Deregisters the activation hook if it is registered.\"\"\"\n",
        "        if self.activation_hook is not None:\n",
        "            self.activation_hook.remove()\n",
        "            self.activation_hook = None\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type, beta1, beta2):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        logger.log(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        logger.log(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        logger.log(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(beta1, beta2), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "def load_tokens(filename):\n",
        "    return torch.load(filename).to(dtype=torch.long)\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, data_dir, B, T, split, shuffle):\n",
        "        self.B, self.T, self.shuffle = B, T, shuffle\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        shards = os.listdir(data_dir)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_dir, s) for s in shards]\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "\n",
        "        logger.log(f\"found {len(shards)} shards for split {split}\")\n",
        "\n",
        "        self.current_shard, self.current_position = -1, 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        global epoch\n",
        "        global saved_lines\n",
        "        epoch += 1\n",
        "        saved_lines = 0\n",
        "        print(f'reset, epoch change {epoch}')\n",
        "        self.current_shard, self.current_position = (self.current_shard + 1) % len(self.shards), 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        if self.shuffle:\n",
        "            start = time.time()\n",
        "            self.shuffle_tokens()\n",
        "            logger.log(f\"shuffled {self.tokens.shape[0]} tokens in {time.time() - start:.1f}s\")\n",
        "\n",
        "    def shuffle_tokens(self, DOCUMENT_END: int = 0):\n",
        "        \"\"\"Shuffle documents in a flat token tensor while keeping each document contiguous.\"\"\"\n",
        "        end_indices = (self.tokens == DOCUMENT_END).nonzero(as_tuple=False).flatten().tolist()\n",
        "\n",
        "        # If the last token is not DOCUMENT_END, consider it as an incomplete document\n",
        "        if not end_indices or end_indices[-1] != len(self.tokens) - 1:\n",
        "            end_indices.append(len(self.tokens) - 1)\n",
        "\n",
        "        documents = []\n",
        "        prev_end = -1  # Start before the first token\n",
        "\n",
        "        for end in end_indices:\n",
        "            # Slice from the token after the previous DOCUMENT_END to the current DOCUMENT_END\n",
        "            # +1 to include the DOCUMENT_END token in the document\n",
        "            doc = self.tokens[prev_end + 1 : end + 1]\n",
        "            documents.append(doc)\n",
        "            prev_end = end\n",
        "\n",
        "        random.shuffle(documents)\n",
        "        self.tokens = torch.cat(documents, dim=0)\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T\n",
        "        # if loading the next batch would be out of bounds, advance to next shard\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.reset()\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def generate(model, enc, prompt, max_length, num_return_sequences):\n",
        "    model.eval()\n",
        "\n",
        "    eos_id = enc.get_vocab()['[EOS]']\n",
        "    tokens = enc.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "    xgen = tokens.to('cuda')\n",
        "    sample_rng = torch.Generator(device='cuda')\n",
        "    sample_rng.manual_seed(42)\n",
        "    while xgen.size(1) < max_length:\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "            # take the logits at the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            # topk_probs here becomes (B, 50), topk_indices is (B, 50)\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "            # select a token from the top-k probabilities\n",
        "            # note: multinomial does not demand the input to sum to 1\n",
        "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "            # append to the sequence\n",
        "            xgen = torch.cat((xgen, xcol), dim=1)\n",
        "    # logger.log the generated text\n",
        "    for i in range(num_return_sequences):\n",
        "        # look for EOS here to truncate.\n",
        "        first_eos = (xgen[i] == eos_id).nonzero()\n",
        "        if first_eos.size(0) > 0:\n",
        "            this_end = first_eos[0].item()\n",
        "        else:\n",
        "            this_end = max_length\n",
        "        tokens = xgen[i, :this_end].tolist()\n",
        "        decoded = enc.decode(tokens)\n",
        "        logger.log(f\"sample {i}: {decoded}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def preprocess_tokens_from_huggingface(dataset_dir):\n",
        "    def flatten_tensorize_dataset_split(it):\n",
        "        num_docs = len(it)\n",
        "        flattened_tokens = []\n",
        "        for doc in tqdm(it, desc='flattening', total=num_docs):\n",
        "            flattened_tokens.extend(doc)\n",
        "        return torch.tensor(flattened_tokens, dtype=torch.int16)\n",
        "\n",
        "    for split in ['validation', 'train']:\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "        fn = f'{dataset_dir}/{split}.pt'\n",
        "        if not os.path.exists(fn):\n",
        "            #logger.log('downloading and processing', split) #real\n",
        "            logger.log('downloading and processing')\n",
        "\n",
        "            #ds = datasets.load_dataset('activated-ai/tiny-stories-8k-tokens', split=split)\n",
        "            #ds = datasets.load_dataset('/my_dataset/', split=split)\n",
        "\n",
        "            features = datasets.Features({\n",
        "        'tokens': datasets.Sequence(datasets.Value(dtype='int32'))  # Changed dtype to int32 if needed\n",
        "    })\n",
        "            ds = datasets.load_dataset('parquet', data_files={'train': 'my_dataset/train/*.parquet', 'validation': 'my_dataset/validation/*.parquet'}, features=features)[split]\n",
        "\n",
        "\n",
        "            val_tensor = flatten_tensorize_dataset_split(ds['tokens'])\n",
        "            torch.save(val_tensor, fn)\n",
        "        else:\n",
        "            logger.log(f'skipping token preprocessing for {split} : using cache {fn}')\n",
        "\n",
        "class ExponentiallyWeightedMean:\n",
        "    def __init__(self, alpha=0.1, skip_first=False):\n",
        "        self.alpha = alpha  # Decay rate\n",
        "        self.mean = None  # The weighted mean\n",
        "        self.skip_first = skip_first  # Whether to skip the first value\n",
        "        self.first_value_skipped = False  # Flag to track if the first value was skipped\n",
        "\n",
        "    def update(self, new_value):\n",
        "        # If we are skipping the first value, ensure we track and skip it\n",
        "        if self.skip_first and not self.first_value_skipped:\n",
        "            self.first_value_skipped = True\n",
        "            return None  # Skip the first value\n",
        "\n",
        "        if self.mean is None:\n",
        "            # If mean is not set, initialize it with the first value\n",
        "            self.mean = new_value\n",
        "        else:\n",
        "            # Update the mean using exponential decay\n",
        "            self.mean = self.alpha * new_value + (1 - self.alpha) * self.mean\n",
        "\n",
        "        return self.mean\n",
        "\n",
        "def load_model_from_checkpoint(checkpoint_path):\n",
        "    def remove_orig_mod_prefix(state_dict):\n",
        "        return {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model = GPT(checkpoint['config'])\n",
        "    model.load_state_dict(remove_orig_mod_prefix(checkpoint['model']))\n",
        "    model.to('cuda')\n",
        "    return model\n",
        "\n",
        "\n",
        "def main():\n",
        "    assert torch.cuda.is_available()\n",
        "    device = \"cuda\"\n",
        "    device_type = \"cuda\"\n",
        "\n",
        "    torch.manual_seed(1337)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(1337)\n",
        "\n",
        "    enc = transformers.AutoTokenizer.from_pretrained('my_tokenizer/')\n",
        "\n",
        "    preprocess_tokens_from_huggingface(config.data_dir)\n",
        "\n",
        "    val_loader = DataLoaderLite(data_dir=config.data_dir, B=config.batch_size, T=config.block_size, split=\"val\", shuffle=False)\n",
        "    bytes_in_val_text = 19190318  # compute this on data load by using tokenizer on say, first 100k tokens in validation data.\n",
        "    bytes_per_token = bytes_in_val_text / val_loader.tokens.shape[0]\n",
        "    if not config.smoke_test:\n",
        "        train_loader = DataLoaderLite(data_dir=config.data_dir , B=config.batch_size, T=config.block_size, split=\"train\", shuffle=config.need_epoch_reshuffle)\n",
        "    else:\n",
        "        train_loader = val_loader\n",
        "\n",
        "    model = GPT(config)\n",
        "\n",
        "    torch.set_float32_matmul_precision(config.matmul_precision)\n",
        "\n",
        "    model.to(device)\n",
        "    use_compile = not config.smoke_test\n",
        "    if use_compile:\n",
        "        logger.log('using torch.compile')\n",
        "        model = torch.compile(model)\n",
        "\n",
        "\n",
        "    def get_lr(it, estimated_steps_in_time_limit=None):\n",
        "        lowest_maximum_steps = min(config.max_steps, estimated_steps_in_time_limit) if estimated_steps_in_time_limit is not None else config.max_steps\n",
        "\n",
        "        # 1) linear warmup for warmup_steps steps\n",
        "        if it < config.warmup_steps:\n",
        "            return config.max_lr * (it + 1) / config.warmup_steps\n",
        "        # 2) if it >= lowest_maximum_steps, return min learning rate\n",
        "        if it >= lowest_maximum_steps:\n",
        "            return config.min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio = (it - config.warmup_steps) / (lowest_maximum_steps - config.warmup_steps)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        decay_ratio = min(max(decay_ratio, 0), 1)  # Ensure decay_ratio is within [0, 1]\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff starts at 1 and goes to 0\n",
        "        return config.min_lr + coeff * (config.max_lr - config.min_lr)\n",
        "\n",
        "\n",
        "    optimizer = model.configure_optimizers(weight_decay=config.weight_decay, learning_rate=config.max_lr,\n",
        "                                           device_type=device_type, beta1=config.beta_1, beta2=config.beta_2)\n",
        "\n",
        "    # create the log directory we will write checkpoints to and log to\n",
        "    log_dir = f'logs/{config.expt_name}'\n",
        "    if config.smoke_test:\n",
        "        log_dir += '_smoke'\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    t_start = time.time()\n",
        "    eval_checkpoint_exit = False\n",
        "    loss_accum = []\n",
        "\n",
        "    # Example usage:\n",
        "    mean_dt_ewma = ExponentiallyWeightedMean(alpha=0.01, skip_first=True)\n",
        "    estimated_steps_in_time_limit = None\n",
        "\n",
        "\n",
        "    for step in range(config.max_steps):\n",
        "        t0 = time.time()\n",
        "        eval_checkpoint_exit = (step == config.max_steps - 1) or eval_checkpoint_exit\n",
        "\n",
        "        # once in a while evaluate our validation loss\n",
        "        if (step % 250 == 0 and step > 0) or eval_checkpoint_exit:\n",
        "            if config.smoke_test:\n",
        "                logger.log('exiting due to smoke test')\n",
        "                eval_checkpoint_exit = True\n",
        "            model.eval()\n",
        "            generate(model, enc, \"Lily went to the park and\", 64, 4)\n",
        "\n",
        "            val_loader.reset()\n",
        "            with torch.no_grad():\n",
        "                val_loss_accum = 0.0\n",
        "                val_loss_steps = 20\n",
        "                for _ in range(val_loss_steps):\n",
        "\n",
        "                    x, y = val_loader.next_batch()\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                        logits, loss = model(x, y)\n",
        "                    loss = loss / val_loss_steps\n",
        "                    val_loss_accum += loss.detach()\n",
        "\n",
        "            val_loss = val_loss_accum.item()\n",
        "            per_byte_loss = val_loss / bytes_per_token\n",
        "\n",
        "            logger.log(f'step {step} | val loss {val_loss:.4f} | byte loss {per_byte_loss:.4f} | ds {time.time() - t_start:.1f}s')\n",
        "\n",
        "            if step > 0 and (step % 5000 == 0 or eval_checkpoint_exit):\n",
        "                # optionally write model checkpoints\n",
        "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'config': model.config,\n",
        "                    'step': step,\n",
        "                    'val_loss': val_loss_accum.item()\n",
        "                }\n",
        "                # Store rng seeds too?\n",
        "\n",
        "                # if last step, save the optimzer state dict\n",
        "                if eval_checkpoint_exit:\n",
        "                    checkpoint['optimizer'] = optimizer.state_dict()\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            if eval_checkpoint_exit:\n",
        "                break\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        loss_accum.append(loss.detach())\n",
        "        loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(step, estimated_steps_in_time_limit=estimated_steps_in_time_limit)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            t1 = time.time()\n",
        "            dt = t1 - t0\n",
        "            ds = t1 - t_start\n",
        "            tokens_processed = train_loader.B * train_loader.T\n",
        "            tokens_per_sec = tokens_processed / dt\n",
        "            avg_loss = sum(loss_accum) / len(loss_accum)\n",
        "            loss_accum.clear()\n",
        "            if ds > config.max_runtime_seconds:\n",
        "                logger.log('exiting due to time limit')\n",
        "                eval_checkpoint_exit = True\n",
        "\n",
        "            mean_dt_ewma.update(dt)\n",
        "            if mean_dt_ewma.mean is not None:\n",
        "                remaining_steps = config.max_steps - step\n",
        "                remaining_time = config.max_runtime_seconds - ds\n",
        "                # let some time for the last step to finish\n",
        "                if (remaining_time < 0) and (remaining_time > -10):\n",
        "                    remaining_time = 0\n",
        "                assert remaining_time >= 0\n",
        "                remaining_steps_in_time_limit = int(remaining_time / mean_dt_ewma.mean)\n",
        "                estimated_steps_in_time_limit = step + remaining_steps_in_time_limit\n",
        "\n",
        "\n",
        "            per_byte_loss = avg_loss / bytes_per_token\n",
        "            logger.log(f'step {step:5d} | loss {avg_loss:.6f} | byte loss {per_byte_loss:.4f} | lr {lr:.4e} | norm {norm:.4f} | dt {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f} | ds {ds:.1f}s')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kw2VWlxijtyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(m, enc, \"Lily went to the park and saw a friendly dog.\", 255, 4)"
      ],
      "metadata": {
        "id": "bDOa_PWgwPSK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "61a2ea56-fb61-4b61-8242-d11485905cea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a82931b78f80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Lily went to the park and saw a friendly dog.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666d3379-1ded-4626-c309-45d66c2c6442"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "def copy_file_from_gdrive(file_name):\n",
        "  \"\"\"Copies a file from Google Drive to the Colab VM.\n",
        "\n",
        "  Args:\n",
        "    file_name: The name of the file to copy.\n",
        "  \"\"\"\n",
        "  # Mount Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "  # Copy the file from Google Drive to the Colab VM\n",
        "  !cp /content/drive/MyDrive/{file_name} /content/\n",
        "\n",
        "def copy_file_to_gdrive(file_name):\n",
        "  \"\"\"Copies a file from the Colab VM to Google Drive.\n",
        "\n",
        "  Args:\n",
        "    file_name: The name of the file to copy.\n",
        "  \"\"\"\n",
        "  # Mount Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "  # Copy the file from the Colab VM to Google Drive\n",
        "  !cp lm_head_inputs/{file_name} /content/drive/MyDrive/\n",
        "\n",
        "# Example usage\n",
        "# To copy large_file.txt from Google Drive to Colab:\n",
        "#copy_file_from_gdrive('large_file.txt')\n",
        "\n",
        "# To copy output_file.txt from Colab to Google Drive:\n",
        "#copy_file_to_gdrive('output_file.txt')\n",
        "\n",
        "#copy_file_to_gdrive('input_epoch_40.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}