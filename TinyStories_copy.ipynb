{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqbitegh/Colabs/blob/main/TinyStories_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "#preparations:\n",
        "#in /dataset put train.pt, validation.pt\n",
        "#/my_dataset/train and /my_dataset/validation put parquet\n",
        "#/my_tokenizer/ put special_tokens_map.json, tokenizer.json, tokenizer_config.json\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQRqZ23jjwsE",
        "outputId": "c3b0c3d6-dda7-4fbd-ecc2-f2badf9399c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"hgf token\""
      ],
      "metadata": {
        "id": "eu2LRNzrm98Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass, fields\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import datasets\n",
        "import tokenizers\n",
        "import transformers  # for AutoTokenizer, using our own transformer implementation.\n",
        "from tqdm import tqdm\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 128 # max sequence length\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 384 # embedding dimension\n",
        "    feed_forward_factor: float = 1.0 #1.8  # how much bigger the MLP blocks are than the model n_embd.  Conventionally 4.\n",
        "    vocab_size: int = 8192\n",
        "\n",
        "    data_dir: str = 'dataset'\n",
        "    expt_name: str = '384_dims_is_all_u_need'\n",
        "\n",
        "    batch_size: int = 128\n",
        "    max_lr: float = 2e-3\n",
        "    min_lr: float = 2e-4\n",
        "    beta_1: float = 0.9\n",
        "    beta_2: float = 0.99\n",
        "    warmup_steps:int = 50\n",
        "    max_steps: int = 12000\n",
        "    max_runtime_seconds: int = 720\n",
        "\n",
        "    weight_decay: float = 0.12\n",
        "\n",
        "    need_epoch_reshuffle: bool = True\n",
        "    matmul_precision: str = 'high' # medium, high, highest.\n",
        "    # Do various hacky things (don't use torch.compile, don't load training data) to speed up the run.\n",
        "    # # We are checking for runnability rather than model quality.\n",
        "    smoke_test: bool = False\n",
        "\n",
        "    def __str__(self):\n",
        "        return '\\n'.join([f'{field.name}: {str(getattr(self, field.name))}' for field in fields(self)])\n",
        "\n",
        "\n",
        "class Logger():\n",
        "    def __init__(self, expt_name, smoke_test):\n",
        "        self.log_dir = f'logs/{expt_name}{\"_smoke\" if smoke_test else \"\"}'\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.log_file = f'{self.log_dir}/log.txt'\n",
        "        with open(self.log_file, \"w\") as f:\n",
        "            f.write(\"\")\n",
        "\n",
        "    def log(self, msg):\n",
        "        print(msg)\n",
        "        with open(self.log_file, \"a\") as f:\n",
        "            f.write(f\"{msg}\\n\")\n",
        "\n",
        "config = GPTConfig()\n",
        "logger = Logger(os.path.join(config.expt_name), config.smoke_test) # open for writing to clear the file\n",
        "logger.log(str(config))\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        ff_exp = int(config.feed_forward_factor * config.n_embd)\n",
        "        ff_exp -= ff_exp % 64\n",
        "        assert ff_exp % 64 == 0\n",
        "        self.c_fc    = nn.Linear(config.n_embd, ff_exp)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(ff_exp, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type, beta1, beta2):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        logger.log(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        logger.log(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        logger.log(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(beta1, beta2), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "def load_tokens(filename):\n",
        "    return torch.load(filename).to(dtype=torch.long)\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, data_dir, B, T, split, shuffle):\n",
        "        self.B, self.T, self.shuffle = B, T, shuffle\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        shards = os.listdir(data_dir)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_dir, s) for s in shards]\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "\n",
        "        logger.log(f\"found {len(shards)} shards for split {split}\")\n",
        "\n",
        "        self.current_shard, self.current_position = -1, 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_shard, self.current_position = (self.current_shard + 1) % len(self.shards), 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        if self.shuffle:\n",
        "            start = time.time()\n",
        "            self.shuffle_tokens()\n",
        "            logger.log(f\"shuffled {self.tokens.shape[0]} tokens in {time.time() - start:.1f}s\")\n",
        "\n",
        "    def shuffle_tokens(self, DOCUMENT_END: int = 0):\n",
        "        \"\"\"Shuffle documents in a flat token tensor while keeping each document contiguous.\"\"\"\n",
        "        end_indices = (self.tokens == DOCUMENT_END).nonzero(as_tuple=False).flatten().tolist()\n",
        "\n",
        "        # If the last token is not DOCUMENT_END, consider it as an incomplete document\n",
        "        if not end_indices or end_indices[-1] != len(self.tokens) - 1:\n",
        "            end_indices.append(len(self.tokens) - 1)\n",
        "\n",
        "        documents = []\n",
        "        prev_end = -1  # Start before the first token\n",
        "\n",
        "        for end in end_indices:\n",
        "            # Slice from the token after the previous DOCUMENT_END to the current DOCUMENT_END\n",
        "            # +1 to include the DOCUMENT_END token in the document\n",
        "            doc = self.tokens[prev_end + 1 : end + 1]\n",
        "            documents.append(doc)\n",
        "            prev_end = end\n",
        "\n",
        "        random.shuffle(documents)\n",
        "        self.tokens = torch.cat(documents, dim=0)\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T\n",
        "        # if loading the next batch would be out of bounds, advance to next shard\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.reset()\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def generate(model, enc, prompt, max_length, num_return_sequences):\n",
        "    model.eval()\n",
        "\n",
        "    eos_id = enc.get_vocab()['[EOS]']\n",
        "    tokens = enc.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "    xgen = tokens.to('cuda')\n",
        "    sample_rng = torch.Generator(device='cuda')\n",
        "    sample_rng.manual_seed(42)\n",
        "    while xgen.size(1) < max_length:\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "            # take the logits at the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            # topk_probs here becomes (B, 50), topk_indices is (B, 50)\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "            # select a token from the top-k probabilities\n",
        "            # note: multinomial does not demand the input to sum to 1\n",
        "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "            # append to the sequence\n",
        "            xgen = torch.cat((xgen, xcol), dim=1)\n",
        "    # logger.log the generated text\n",
        "    for i in range(num_return_sequences):\n",
        "        # look for EOS here to truncate.\n",
        "        first_eos = (xgen[i] == eos_id).nonzero()\n",
        "        if first_eos.size(0) > 0:\n",
        "            this_end = first_eos[0].item()\n",
        "        else:\n",
        "            this_end = max_length\n",
        "        tokens = xgen[i, :this_end].tolist()\n",
        "        decoded = enc.decode(tokens)\n",
        "        logger.log(f\"sample {i}: {decoded}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def preprocess_tokens_from_huggingface(dataset_dir):\n",
        "    def flatten_tensorize_dataset_split(it):\n",
        "        num_docs = len(it)\n",
        "        flattened_tokens = []\n",
        "        for doc in tqdm(it, desc='flattening', total=num_docs):\n",
        "            flattened_tokens.extend(doc)\n",
        "        return torch.tensor(flattened_tokens, dtype=torch.int16)\n",
        "\n",
        "    for split in ['validation', 'train']:\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "        fn = f'{dataset_dir}/{split}.pt'\n",
        "        if not os.path.exists(fn):\n",
        "            #logger.log('downloading and processing', split) #real\n",
        "            logger.log('downloading and processing')\n",
        "\n",
        "            #ds = datasets.load_dataset('activated-ai/tiny-stories-8k-tokens', split=split)\n",
        "            #ds = datasets.load_dataset('/my_dataset/', split=split)\n",
        "\n",
        "            features = datasets.Features({\n",
        "        'tokens': datasets.Sequence(datasets.Value(dtype='int32'))  # Changed dtype to int32 if needed\n",
        "    })\n",
        "            ds = datasets.load_dataset('parquet', data_files={'train': 'my_dataset/train/*.parquet', 'validation': 'my_dataset/validation/*.parquet'}, features=features)[split]\n",
        "\n",
        "\n",
        "            val_tensor = flatten_tensorize_dataset_split(ds['tokens'])\n",
        "            torch.save(val_tensor, fn)\n",
        "        else:\n",
        "            logger.log(f'skipping token preprocessing for {split} : using cache {fn}')\n",
        "\n",
        "class ExponentiallyWeightedMean:\n",
        "    def __init__(self, alpha=0.1, skip_first=False):\n",
        "        self.alpha = alpha  # Decay rate\n",
        "        self.mean = None  # The weighted mean\n",
        "        self.skip_first = skip_first  # Whether to skip the first value\n",
        "        self.first_value_skipped = False  # Flag to track if the first value was skipped\n",
        "\n",
        "    def update(self, new_value):\n",
        "        # If we are skipping the first value, ensure we track and skip it\n",
        "        if self.skip_first and not self.first_value_skipped:\n",
        "            self.first_value_skipped = True\n",
        "            return None  # Skip the first value\n",
        "\n",
        "        if self.mean is None:\n",
        "            # If mean is not set, initialize it with the first value\n",
        "            self.mean = new_value\n",
        "        else:\n",
        "            # Update the mean using exponential decay\n",
        "            self.mean = self.alpha * new_value + (1 - self.alpha) * self.mean\n",
        "\n",
        "        return self.mean\n",
        "\n",
        "def load_model_from_checkpoint(checkpoint_path):\n",
        "    def remove_orig_mod_prefix(state_dict):\n",
        "        return {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model = GPT(checkpoint['config'])\n",
        "    model.load_state_dict(remove_orig_mod_prefix(checkpoint['model']))\n",
        "    model.to('cuda')\n",
        "    return model\n",
        "\n",
        "\n",
        "def main():\n",
        "    assert torch.cuda.is_available()\n",
        "    device = \"cuda\"\n",
        "    device_type = \"cuda\"\n",
        "\n",
        "    torch.manual_seed(1337)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(1337)\n",
        "\n",
        "    enc = transformers.AutoTokenizer.from_pretrained('my_tokenizer/')\n",
        "\n",
        "    preprocess_tokens_from_huggingface(config.data_dir)\n",
        "\n",
        "    val_loader = DataLoaderLite(data_dir=config.data_dir, B=config.batch_size, T=config.block_size, split=\"val\", shuffle=False)\n",
        "    bytes_in_val_text = 19190318  # compute this on data load by using tokenizer on say, first 100k tokens in validation data.\n",
        "    bytes_per_token = bytes_in_val_text / val_loader.tokens.shape[0]\n",
        "    if not config.smoke_test:\n",
        "        train_loader = DataLoaderLite(data_dir=config.data_dir , B=config.batch_size, T=config.block_size, split=\"train\", shuffle=config.need_epoch_reshuffle)\n",
        "    else:\n",
        "        train_loader = val_loader\n",
        "\n",
        "    model = GPT(config)\n",
        "\n",
        "    torch.set_float32_matmul_precision(config.matmul_precision)\n",
        "\n",
        "    model.to(device)\n",
        "    use_compile = not config.smoke_test\n",
        "    if use_compile:\n",
        "        logger.log('using torch.compile')\n",
        "        model = torch.compile(model)\n",
        "\n",
        "\n",
        "    def get_lr(it, estimated_steps_in_time_limit=None):\n",
        "        lowest_maximum_steps = min(config.max_steps, estimated_steps_in_time_limit) if estimated_steps_in_time_limit is not None else config.max_steps\n",
        "\n",
        "        # 1) linear warmup for warmup_steps steps\n",
        "        if it < config.warmup_steps:\n",
        "            return config.max_lr * (it + 1) / config.warmup_steps\n",
        "        # 2) if it >= lowest_maximum_steps, return min learning rate\n",
        "        if it >= lowest_maximum_steps:\n",
        "            return config.min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio = (it - config.warmup_steps) / (lowest_maximum_steps - config.warmup_steps)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        decay_ratio = min(max(decay_ratio, 0), 1)  # Ensure decay_ratio is within [0, 1]\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff starts at 1 and goes to 0\n",
        "        return config.min_lr + coeff * (config.max_lr - config.min_lr)\n",
        "\n",
        "\n",
        "    optimizer = model.configure_optimizers(weight_decay=config.weight_decay, learning_rate=config.max_lr,\n",
        "                                           device_type=device_type, beta1=config.beta_1, beta2=config.beta_2)\n",
        "\n",
        "    # create the log directory we will write checkpoints to and log to\n",
        "    log_dir = f'logs/{config.expt_name}'\n",
        "    if config.smoke_test:\n",
        "        log_dir += '_smoke'\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    t_start = time.time()\n",
        "    eval_checkpoint_exit = False\n",
        "    loss_accum = []\n",
        "\n",
        "    # Example usage:\n",
        "    mean_dt_ewma = ExponentiallyWeightedMean(alpha=0.01, skip_first=True)\n",
        "    estimated_steps_in_time_limit = None\n",
        "\n",
        "\n",
        "    for step in range(config.max_steps):\n",
        "        t0 = time.time()\n",
        "        eval_checkpoint_exit = (step == config.max_steps - 1) or eval_checkpoint_exit\n",
        "\n",
        "        # once in a while evaluate our validation loss\n",
        "        if (step % 250 == 0 and step > 0) or eval_checkpoint_exit:\n",
        "            if config.smoke_test:\n",
        "                logger.log('exiting due to smoke test')\n",
        "                eval_checkpoint_exit = True\n",
        "            model.eval()\n",
        "            generate(model, enc, \"Lily went to the park and\", 64, 4)\n",
        "\n",
        "            val_loader.reset()\n",
        "            with torch.no_grad():\n",
        "                val_loss_accum = 0.0\n",
        "                val_loss_steps = 20\n",
        "                for _ in range(val_loss_steps):\n",
        "\n",
        "                    x, y = val_loader.next_batch()\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                        logits, loss = model(x, y)\n",
        "                    loss = loss / val_loss_steps\n",
        "                    val_loss_accum += loss.detach()\n",
        "\n",
        "            val_loss = val_loss_accum.item()\n",
        "            per_byte_loss = val_loss / bytes_per_token\n",
        "\n",
        "            logger.log(f'step {step} | val loss {val_loss:.4f} | byte loss {per_byte_loss:.4f} | ds {time.time() - t_start:.1f}s')\n",
        "\n",
        "            if step > 0 and (step % 5000 == 0 or eval_checkpoint_exit):\n",
        "                # optionally write model checkpoints\n",
        "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'config': model.config,\n",
        "                    'step': step,\n",
        "                    'val_loss': val_loss_accum.item()\n",
        "                }\n",
        "                # Store rng seeds too?\n",
        "\n",
        "                # if last step, save the optimzer state dict\n",
        "                if eval_checkpoint_exit:\n",
        "                    checkpoint['optimizer'] = optimizer.state_dict()\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            if eval_checkpoint_exit:\n",
        "                break\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        loss_accum.append(loss.detach())\n",
        "        loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(step, estimated_steps_in_time_limit=estimated_steps_in_time_limit)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            t1 = time.time()\n",
        "            dt = t1 - t0\n",
        "            ds = t1 - t_start\n",
        "            tokens_processed = train_loader.B * train_loader.T\n",
        "            tokens_per_sec = tokens_processed / dt\n",
        "            avg_loss = sum(loss_accum) / len(loss_accum)\n",
        "            loss_accum.clear()\n",
        "            if ds > config.max_runtime_seconds:\n",
        "                logger.log('exiting due to time limit')\n",
        "                eval_checkpoint_exit = True\n",
        "\n",
        "            mean_dt_ewma.update(dt)\n",
        "            if mean_dt_ewma.mean is not None:\n",
        "                remaining_steps = config.max_steps - step\n",
        "                remaining_time = config.max_runtime_seconds - ds\n",
        "                # let some time for the last step to finish\n",
        "                if (remaining_time < 0) and (remaining_time > -10):\n",
        "                    remaining_time = 0\n",
        "                assert remaining_time >= 0\n",
        "                remaining_steps_in_time_limit = int(remaining_time / mean_dt_ewma.mean)\n",
        "                estimated_steps_in_time_limit = step + remaining_steps_in_time_limit\n",
        "\n",
        "\n",
        "            per_byte_loss = avg_loss / bytes_per_token\n",
        "            logger.log(f'step {step:5d} | loss {avg_loss:.6f} | byte loss {per_byte_loss:.4f} | lr {lr:.4e} | norm {norm:.4f} | dt {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f} | ds {ds:.1f}s')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kw2VWlxijtyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb90b6ad-952f-4974-98b1-f9e0a79eabd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block_size: 128\n",
            "n_layer: 12\n",
            "n_head: 12\n",
            "n_embd: 384\n",
            "feed_forward_factor: 1.0\n",
            "vocab_size: 8192\n",
            "data_dir: dataset\n",
            "expt_name: 384_dims_is_all_u_need\n",
            "batch_size: 128\n",
            "max_lr: 0.002\n",
            "min_lr: 0.0002\n",
            "beta_1: 0.9\n",
            "beta_2: 0.99\n",
            "warmup_steps: 50\n",
            "max_steps: 12000\n",
            "max_runtime_seconds: 720\n",
            "weight_decay: 0.12\n",
            "need_epoch_reshuffle: True\n",
            "matmul_precision: high\n",
            "smoke_test: False\n",
            "skipping token preprocessing for validation : using cache dataset/validation.pt\n",
            "skipping token preprocessing for train : using cache dataset/train.pt\n",
            "found 1 shards for split val\n",
            "found 1 shards for split train\n",
            "shuffled 117256624 tokens in 4.9s\n",
            "using torch.compile\n",
            "num decayed parameter tensors: 50, with 13,811,712 parameters\n",
            "num non-decayed parameter tensors: 98, with 46,848 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step     0 | loss 9.012011 | byte loss 2.2030 | lr 4.0000e-05 | norm 15.4235 | dt 19523.35ms | tok/sec: 839.20 | ds 19.5s\n",
            "step    10 | loss 8.082259 | byte loss 1.9758 | lr 4.4000e-04 | norm 142.3386 | dt 768.15ms | tok/sec: 21329.04 | ds 26.8s\n",
            "step    20 | loss 6.416646 | byte loss 1.5686 | lr 8.4000e-04 | norm 0.6964 | dt 774.81ms | tok/sec: 21145.90 | ds 34.5s\n",
            "step    30 | loss 5.770136 | byte loss 1.4105 | lr 1.2400e-03 | norm 1.3204 | dt 785.51ms | tok/sec: 20857.72 | ds 42.3s\n",
            "step    40 | loss 5.488723 | byte loss 1.3418 | lr 1.6400e-03 | norm 0.7028 | dt 794.60ms | tok/sec: 20619.08 | ds 50.3s\n",
            "step    50 | loss 5.274492 | byte loss 1.2894 | lr 2.0000e-03 | norm 1.4263 | dt 801.85ms | tok/sec: 20432.70 | ds 58.3s\n",
            "step    60 | loss 5.009733 | byte loss 1.2247 | lr 1.9994e-03 | norm 0.9430 | dt 789.84ms | tok/sec: 20743.55 | ds 66.2s\n",
            "step    70 | loss 4.740037 | byte loss 1.1587 | lr 1.9976e-03 | norm 0.7491 | dt 782.74ms | tok/sec: 20931.58 | ds 74.1s\n",
            "step    80 | loss 4.551166 | byte loss 1.1126 | lr 1.9946e-03 | norm 0.6263 | dt 779.63ms | tok/sec: 21014.96 | ds 81.9s\n",
            "step    90 | loss 4.419419 | byte loss 1.0804 | lr 1.9904e-03 | norm 0.6808 | dt 773.07ms | tok/sec: 21193.38 | ds 89.6s\n",
            "step   100 | loss 4.307970 | byte loss 1.0531 | lr 1.9850e-03 | norm 0.6859 | dt 813.25ms | tok/sec: 20146.36 | ds 97.4s\n",
            "step   110 | loss 4.210601 | byte loss 1.0293 | lr 1.9784e-03 | norm 0.4952 | dt 775.62ms | tok/sec: 21123.67 | ds 105.1s\n",
            "step   120 | loss 4.137817 | byte loss 1.0115 | lr 1.9706e-03 | norm 0.6793 | dt 778.51ms | tok/sec: 21045.31 | ds 112.9s\n",
            "step   130 | loss 4.053178 | byte loss 0.9908 | lr 1.9617e-03 | norm 0.6107 | dt 784.10ms | tok/sec: 20895.36 | ds 120.7s\n",
            "step   140 | loss 3.966119 | byte loss 0.9695 | lr 1.9516e-03 | norm 0.5728 | dt 788.10ms | tok/sec: 20789.26 | ds 128.6s\n",
            "step   150 | loss 3.916301 | byte loss 0.9574 | lr 1.9402e-03 | norm 0.6111 | dt 789.69ms | tok/sec: 20747.43 | ds 136.5s\n",
            "step   160 | loss 3.843012 | byte loss 0.9394 | lr 1.9278e-03 | norm 0.7979 | dt 789.23ms | tok/sec: 20759.38 | ds 144.4s\n",
            "step   170 | loss 3.804757 | byte loss 0.9301 | lr 1.9141e-03 | norm 0.6814 | dt 783.19ms | tok/sec: 20919.68 | ds 152.2s\n",
            "step   180 | loss 3.749080 | byte loss 0.9165 | lr 1.8995e-03 | norm 0.5097 | dt 781.11ms | tok/sec: 20975.36 | ds 160.1s\n",
            "step   190 | loss 3.696598 | byte loss 0.9037 | lr 1.8838e-03 | norm 0.5684 | dt 781.09ms | tok/sec: 20975.81 | ds 167.9s\n",
            "step   200 | loss 3.652730 | byte loss 0.8929 | lr 1.8670e-03 | norm 0.9740 | dt 776.67ms | tok/sec: 21095.16 | ds 175.7s\n",
            "step   210 | loss 3.595585 | byte loss 0.8790 | lr 1.8489e-03 | norm 0.5969 | dt 779.67ms | tok/sec: 21014.08 | ds 183.5s\n",
            "step   220 | loss 3.521466 | byte loss 0.8608 | lr 1.8301e-03 | norm 0.5251 | dt 783.00ms | tok/sec: 20924.73 | ds 191.3s\n",
            "step   230 | loss 3.504796 | byte loss 0.8568 | lr 1.8102e-03 | norm 0.5167 | dt 780.58ms | tok/sec: 20989.46 | ds 199.2s\n",
            "step   240 | loss 3.422726 | byte loss 0.8367 | lr 1.7894e-03 | norm 0.5759 | dt 782.07ms | tok/sec: 20949.62 | ds 207.0s\n",
            "sample 0: Lily went to the park and saw a little boy named Jack's head. Jack was excited and scared if his mom told him it. His mom smiled and said he decided to do for your toy.\n",
            "\n",
            "Jack came to play with him from the park with Tom to the kitchen. He ran out of the trees and\n",
            "sample 1: Lily went to the park and talked the shop. The end of mud and laughed her head was time for their day and her. Her family enjoyed the wonderful day and had helped them warm.\n",
            "sample 2: Lily went to the park and had a magic bite. \"Sure, we am mine!\" she smiled and said. \"We are sorry, Mom!\"\n",
            "\n",
            "Lily felt happy and they both said, \"We can be more new. We just can each love their toys. You are glad we have a new friends and\n",
            "sample 3: Lily went to the park and said they were playing with their room.\n",
            "step 250 | val loss 3.4219 | byte loss 0.8365 | ds 221.3s\n",
            "step   250 | loss 3.405861 | byte loss 0.8326 | lr 1.7672e-03 | norm 0.5538 | dt 7338.20ms | tok/sec: 2232.70 | ds 221.4s\n",
            "step   260 | loss 3.355793 | byte loss 0.8203 | lr 1.7075e-03 | norm 0.4891 | dt 784.17ms | tok/sec: 20893.46 | ds 229.2s\n",
            "step   270 | loss 3.337067 | byte loss 0.8158 | lr 1.6815e-03 | norm 0.5140 | dt 784.83ms | tok/sec: 20875.86 | ds 237.0s\n",
            "step   280 | loss 3.297495 | byte loss 0.8061 | lr 1.6548e-03 | norm 0.4992 | dt 785.28ms | tok/sec: 20863.87 | ds 244.9s\n",
            "step   290 | loss 3.242756 | byte loss 0.7927 | lr 1.6273e-03 | norm 0.4753 | dt 784.93ms | tok/sec: 20873.15 | ds 252.7s\n",
            "step   300 | loss 3.206799 | byte loss 0.7839 | lr 1.5991e-03 | norm 0.5642 | dt 782.29ms | tok/sec: 20943.76 | ds 260.6s\n",
            "step   310 | loss 3.206937 | byte loss 0.7840 | lr 1.5702e-03 | norm 0.5234 | dt 784.22ms | tok/sec: 20892.19 | ds 268.5s\n",
            "step   320 | loss 3.182247 | byte loss 0.7779 | lr 1.5408e-03 | norm 0.5193 | dt 783.39ms | tok/sec: 20914.17 | ds 276.3s\n",
            "step   330 | loss 3.148860 | byte loss 0.7698 | lr 1.5108e-03 | norm 0.5230 | dt 785.22ms | tok/sec: 20865.36 | ds 284.1s\n",
            "step   340 | loss 3.146213 | byte loss 0.7691 | lr 1.4803e-03 | norm 0.4388 | dt 783.80ms | tok/sec: 20903.24 | ds 292.0s\n",
            "step   350 | loss 3.082164 | byte loss 0.7535 | lr 1.4493e-03 | norm 0.5136 | dt 785.14ms | tok/sec: 20867.51 | ds 299.8s\n",
            "step   360 | loss 3.046491 | byte loss 0.7447 | lr 1.4166e-03 | norm 0.5486 | dt 784.75ms | tok/sec: 20877.90 | ds 307.6s\n",
            "step   370 | loss 3.056621 | byte loss 0.7472 | lr 1.3848e-03 | norm 0.4842 | dt 786.17ms | tok/sec: 20840.16 | ds 315.5s\n",
            "step   380 | loss 3.062728 | byte loss 0.7487 | lr 1.3526e-03 | norm 0.4733 | dt 784.47ms | tok/sec: 20885.53 | ds 323.3s\n",
            "step   390 | loss 3.016148 | byte loss 0.7373 | lr 1.3202e-03 | norm 0.4939 | dt 785.02ms | tok/sec: 20870.76 | ds 331.1s\n",
            "step   400 | loss 3.019104 | byte loss 0.7380 | lr 1.2876e-03 | norm 0.4673 | dt 781.99ms | tok/sec: 20951.60 | ds 339.0s\n",
            "step   410 | loss 2.966254 | byte loss 0.7251 | lr 1.2548e-03 | norm 0.4804 | dt 782.18ms | tok/sec: 20946.54 | ds 346.8s\n",
            "step   420 | loss 2.993127 | byte loss 0.7317 | lr 1.2218e-03 | norm 0.4421 | dt 782.69ms | tok/sec: 20932.82 | ds 354.6s\n",
            "step   430 | loss 2.931957 | byte loss 0.7167 | lr 1.1871e-03 | norm 0.4728 | dt 780.98ms | tok/sec: 20978.64 | ds 362.4s\n",
            "step   440 | loss 2.931919 | byte loss 0.7167 | lr 1.1540e-03 | norm 0.5167 | dt 780.05ms | tok/sec: 21003.75 | ds 370.2s\n",
            "step   450 | loss 2.917393 | byte loss 0.7132 | lr 1.1209e-03 | norm 0.4647 | dt 784.07ms | tok/sec: 20896.14 | ds 378.1s\n",
            "step   460 | loss 2.885140 | byte loss 0.7053 | lr 1.0878e-03 | norm 0.4216 | dt 782.31ms | tok/sec: 20943.15 | ds 385.9s\n",
            "step   470 | loss 2.880828 | byte loss 0.7042 | lr 1.0531e-03 | norm 0.4094 | dt 781.77ms | tok/sec: 20957.56 | ds 393.7s\n",
            "step   480 | loss 2.855857 | byte loss 0.6981 | lr 1.0202e-03 | norm 0.4516 | dt 783.50ms | tok/sec: 20911.20 | ds 401.5s\n",
            "step   490 | loss 2.831179 | byte loss 0.6921 | lr 9.8754e-04 | norm 0.4412 | dt 784.03ms | tok/sec: 20897.25 | ds 409.4s\n",
            "sample 0: Lily went to the park and saw a squirrel. He picked his friend because it was hidden and looked yummy!\n",
            "\n",
            "Lily wanted to be a new friend. She felt like she needed a surprise to take a special letter. When she was at the park, Tommy got a bit stuck in the tree. He could barely\n",
            "sample 1: Lily went to the park and met no expensive. She gave the cheese to Lily back. Lily liked her very much and loved it that they had lots of fun.\n",
            "\n",
            "Lily sat on the swings and the plants. She liked the leaves and swings and the leaves. When, the sun went down and Lily's swing\n",
            "sample 2: Lily went to the park and said, \"OK, Mommy. Do you think it is my toy?\"\n",
            "\n",
            "The boy smiled and said, \"Yes, I am a little boy. I want to play a game.\"\n",
            "sample 3: Lily went to the park and saw Spot. Spot was happy to play. She hugged him and said, \"Please, Mr, okay. We're sorry. We were just smart. We should share it and help.\"\n",
            "\n",
            "Tom and Anna went back to the park. They saw some different toys, and words on\n",
            "step 500 | val loss 2.8321 | byte loss 0.6923 | ds 423.6s\n",
            "step   500 | loss 2.835439 | byte loss 0.6931 | lr 9.5510e-04 | norm 0.4389 | dt 7297.32ms | tok/sec: 2245.21 | ds 423.7s\n",
            "step   510 | loss 2.794443 | byte loss 0.6831 | lr 8.5370e-04 | norm 0.4416 | dt 784.30ms | tok/sec: 20890.02 | ds 431.5s\n",
            "step   520 | loss 2.801205 | byte loss 0.6848 | lr 8.2326e-04 | norm 0.4443 | dt 781.04ms | tok/sec: 20977.20 | ds 439.3s\n",
            "step   530 | loss 2.758842 | byte loss 0.6744 | lr 7.9124e-04 | norm 0.4543 | dt 779.70ms | tok/sec: 21013.09 | ds 447.1s\n",
            "step   540 | loss 2.774945 | byte loss 0.6784 | lr 7.6182e-04 | norm 0.4496 | dt 781.43ms | tok/sec: 20966.79 | ds 455.0s\n",
            "step   550 | loss 2.754336 | byte loss 0.6733 | lr 7.3296e-04 | norm 0.4472 | dt 783.30ms | tok/sec: 20916.53 | ds 462.8s\n",
            "step   560 | loss 2.748785 | byte loss 0.6720 | lr 7.0261e-04 | norm 0.4274 | dt 782.22ms | tok/sec: 20945.47 | ds 470.6s\n",
            "step   570 | loss 2.706975 | byte loss 0.6617 | lr 6.7496e-04 | norm 0.4204 | dt 779.93ms | tok/sec: 21007.00 | ds 478.4s\n",
            "step   580 | loss 2.706782 | byte loss 0.6617 | lr 6.4589e-04 | norm 0.4287 | dt 783.33ms | tok/sec: 20915.78 | ds 486.2s\n",
            "step   590 | loss 2.703074 | byte loss 0.6608 | lr 6.1956e-04 | norm 0.4034 | dt 779.88ms | tok/sec: 21008.44 | ds 494.0s\n",
            "step   600 | loss 2.716560 | byte loss 0.6641 | lr 5.9189e-04 | norm 0.4265 | dt 782.33ms | tok/sec: 20942.49 | ds 501.8s\n",
            "step   610 | loss 2.697747 | byte loss 0.6595 | lr 5.6498e-04 | norm 0.4149 | dt 780.32ms | tok/sec: 20996.40 | ds 509.6s\n",
            "step   620 | loss 2.668987 | byte loss 0.6525 | lr 5.4084e-04 | norm 0.4338 | dt 780.19ms | tok/sec: 20999.93 | ds 517.5s\n",
            "step   630 | loss 2.659102 | byte loss 0.6500 | lr 5.1550e-04 | norm 0.4282 | dt 783.07ms | tok/sec: 20922.74 | ds 525.3s\n",
            "step   640 | loss 2.679916 | byte loss 0.6551 | lr 4.9291e-04 | norm 0.4251 | dt 779.79ms | tok/sec: 21010.77 | ds 533.1s\n",
            "step   650 | loss 2.640933 | byte loss 0.6456 | lr 4.6923e-04 | norm 0.4174 | dt 783.14ms | tok/sec: 20920.81 | ds 540.9s\n",
            "step   660 | loss 2.633458 | byte loss 0.6438 | lr 4.4644e-04 | norm 0.4191 | dt 784.28ms | tok/sec: 20890.43 | ds 548.7s\n",
            "step   670 | loss 2.653533 | byte loss 0.6487 | lr 4.2455e-04 | norm 0.4453 | dt 783.64ms | tok/sec: 20907.64 | ds 556.6s\n",
            "step   680 | loss 2.641429 | byte loss 0.6457 | lr 4.0533e-04 | norm 0.4340 | dt 783.64ms | tok/sec: 20907.43 | ds 564.4s\n",
            "step   690 | loss 2.615593 | byte loss 0.6394 | lr 3.8527e-04 | norm 0.4209 | dt 782.29ms | tok/sec: 20943.75 | ds 572.2s\n",
            "step   700 | loss 2.617594 | byte loss 0.6399 | lr 3.6618e-04 | norm 0.4314 | dt 784.77ms | tok/sec: 20877.52 | ds 580.0s\n",
            "step   710 | loss 2.579775 | byte loss 0.6306 | lr 3.4806e-04 | norm 0.4152 | dt 783.82ms | tok/sec: 20902.70 | ds 587.9s\n",
            "step   720 | loss 2.585931 | byte loss 0.6321 | lr 3.3094e-04 | norm 0.4188 | dt 780.38ms | tok/sec: 20994.86 | ds 595.7s\n",
            "step   730 | loss 2.572818 | byte loss 0.6289 | lr 3.1627e-04 | norm 0.4194 | dt 782.61ms | tok/sec: 20935.10 | ds 603.5s\n",
            "step   740 | loss 2.585313 | byte loss 0.6320 | lr 3.0111e-04 | norm 0.3996 | dt 784.38ms | tok/sec: 20887.91 | ds 611.3s\n",
            "sample 0: Lily went to the park and saw a lot of people waiting for them. She picked them and went to tell them what they needed to buy. \n",
            "\n",
            "Lily's mom told them the kind man they had to find it. Lily smiled. She was happy she and ran to play with the police. She had a nice\n",
            "sample 1: Lily went to the park and her mom saw the new toy car. She was about to go home. Lily's friend said, \"Mom, I'm sorry. Can you come running home?\" Her mom said, \"Yes, you can. I will have some money on?\" They went to the store to play with\n",
            "sample 2: Lily went to the park and found a squirrel sitting on a bench. She walked and ran after him until she saw the squirrel. She was scared, but she knew she had to be quiet. The squirrel said that Lily had help, but for a little girl at the park.\n",
            "\n",
            "Lily went back to the vet\n",
            "sample 3: Lily went to the park and saw her mom, \"Mommy! Your doll helps us make you angry!\" Her mom took her to the park. They put on their coats and carried them to the swings.\n",
            "\n",
            "When they saw the dog Lily's mom said, \"Lily, you're too busy! You caught the\n",
            "step 750 | val loss 2.5921 | byte loss 0.6337 | ds 625.6s\n",
            "step   750 | loss 2.585839 | byte loss 0.6321 | lr 2.8699e-04 | norm 0.4197 | dt 7312.67ms | tok/sec: 2240.49 | ds 625.7s\n",
            "step   760 | loss 2.561560 | byte loss 0.6262 | lr 2.5672e-04 | norm 0.4124 | dt 782.34ms | tok/sec: 20942.31 | ds 633.5s\n",
            "step   770 | loss 2.569486 | byte loss 0.6281 | lr 2.4603e-04 | norm 0.4230 | dt 784.21ms | tok/sec: 20892.43 | ds 641.4s\n",
            "step   780 | loss 2.573949 | byte loss 0.6292 | lr 2.3736e-04 | norm 0.4096 | dt 781.10ms | tok/sec: 20975.45 | ds 649.2s\n",
            "step   790 | loss 2.552842 | byte loss 0.6241 | lr 2.2962e-04 | norm 0.4241 | dt 782.48ms | tok/sec: 20938.46 | ds 657.0s\n",
            "step   800 | loss 2.554991 | byte loss 0.6246 | lr 2.2279e-04 | norm 0.4172 | dt 782.90ms | tok/sec: 20927.33 | ds 664.9s\n",
            "step   810 | loss 2.512309 | byte loss 0.6141 | lr 2.1624e-04 | norm 0.4276 | dt 786.53ms | tok/sec: 20830.78 | ds 672.7s\n",
            "step   820 | loss 2.528950 | byte loss 0.6182 | lr 2.1133e-04 | norm 0.4235 | dt 783.19ms | tok/sec: 20919.60 | ds 680.5s\n",
            "step   830 | loss 2.532487 | byte loss 0.6191 | lr 2.0689e-04 | norm 0.4085 | dt 783.08ms | tok/sec: 20922.53 | ds 688.4s\n",
            "step   840 | loss 2.536028 | byte loss 0.6199 | lr 2.0386e-04 | norm 0.4513 | dt 785.94ms | tok/sec: 20846.40 | ds 696.2s\n",
            "step   850 | loss 2.548154 | byte loss 0.6229 | lr 2.0150e-04 | norm 0.4050 | dt 779.83ms | tok/sec: 21009.76 | ds 704.0s\n",
            "step   860 | loss 2.536723 | byte loss 0.6201 | lr 2.0033e-04 | norm 0.4163 | dt 783.04ms | tok/sec: 20923.47 | ds 711.9s\n",
            "step   870 | loss 2.529739 | byte loss 0.6184 | lr 2.0000e-04 | norm 0.4408 | dt 785.17ms | tok/sec: 20866.91 | ds 719.7s\n",
            "exiting due to time limit\n",
            "step   880 | loss 2.501647 | byte loss 0.6115 | lr 2.0000e-04 | norm 0.4193 | dt 785.69ms | tok/sec: 20852.91 | ds 727.6s\n",
            "sample 0: Lily went to the park and saw a squirrel. She loved him very much. She wanted to ride one by the leash, so she walked up to him. \n",
            "\n",
            "But then, the dog accidentally dropped the gun and it flew too fast. Lily was so little. She was scared and hurt her leg. \n",
            "\n",
            "\n",
            "sample 1: Lily went to the park and found a big, green butterfly on it. It took one and put it inside the pretty feathers. As Lily took a bite, the caterpillar began to giggle. It was a very big, loud and clear and warm, but it had no way for it. Lily was so excited that she\n",
            "sample 2: Lily went to the park and found a soft-looking bug. The bug loved to play near his tail and take it away.\n",
            "\n",
            "\"Mom, can we play with the bug?\" Lily asked.\n",
            "\n",
            "\"No, we can't. We need your nut,\" Tom said.\n",
            "\n",
            "Mom laughed and said\n",
            "sample 3: Lily went to the park and saw many animals. She saw many bugs, boats, and colorful colors. Lily felt excited and happy.\n",
            "\n",
            "Mommy said to Lily, \"There, look, we're going to see how we go!\" Mommy nodded and said, \"Yes, we can play!\"\n",
            "step 881 | val loss 2.5394 | byte loss 0.6208 | ds 734.8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(m, enc, \"Lily went to the park and saw a friendly dog.\", 255, 4)"
      ],
      "metadata": {
        "id": "bDOa_PWgwPSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "copy_file_from_gdrive(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}